<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title><![CDATA[T检验]]></title>
      <url>/2018/01/22/T%E6%A3%80%E9%AA%8C/</url>
      <content type="html"><![CDATA[<h1>各种检验的来源</h1>
<p>为了确定从样本(sample)统计结果推论至总体时所犯错的概率，我们会利用统计学家所开发的一些统计方法，进行统计检定。</p>
<p>通过把所得到的统计检定值，与统计学家建立了一些随机变量的概率分布(probability distribution)进行比较，我们可以知道在多少%的机会下会得到目前的结果。倘若经比较后发现，出现这结果的机率很少，亦即是说，是在机会很少、很罕有的情况下才出现；那我们便可以有信心的说，这不是巧合，是具有统计学上的意义的(用统计学的话讲，就是能够拒绝虚无假设null hypothesis,Ho)。相反，若比较后发现，出现的机率很高，并不罕见；那我们便不能很有信心的直指这不是巧合，也许是巧合，也许不是，但我们没能确定。</p>
<p>F值和t值就是这些统计检定值，与它们相对应的概率分布，就是F分布和t分布。统计显著性（sig）就是出现目前样本这结果的机率。</p>
<h1>统计学意义（P值或sig值）</h1>
<p>结果的统计学意义是结果真实程度（能够代表总体）的一种估计方法。专业上，p值为结果可信程度的一个递减指标，p值越大，我们越不能认为样本中变量的关联是总体中各变量关联的可靠指标。p值是将观察结果认为有效即具有总体代表性的犯错概率。如p=0.05提示样本中变量关联有5%的可能是由于偶然性造成的。即假设总体中任意变量间均无关联，我们重复类似实验，会发现约20个实验中有一个实验，我们所研究的变量关联将等于或强于我们的实验结果。（这并不是说如果变量间存在关联，我们可得到5%或95%次数的相同结果，当总体中的变量存在关联，重复研究和发现关联的可能性与设计的统计学效力有关。）在许多研究领域，0.05的p值通常被认为是可接受错误的边界水平。</p>
<h1>t检验</h1>
<p>T检验，亦称student t检验（Student's t test），主要用于样本含量较小（例如n&lt;30），总体标准差σ未知的正态分布。</p>
<h2>适用条件</h2>
<ol>
<li>已知一个总体均数；</li>
<li>可得到一个样本均数及该样本标准差；</li>
<li>样本来自正态或近似正态总体。</li>
</ol>
<h2>分类</h2>
<p>t检验分为单总体检验和双总体检验。</p>
<p>单总体t检验是检验一个样本平均数与一个已知的总体平均数的差异是否显著。当总体分布是正态分布，如总体标准差未知且样本容量小于30，那么样本平均数与总体平均数的离差统计量呈t分布。</p>
<p>双总体t检验是检验两个样本平均数与其各自所代表的总体的差异是否显著。双总体t检验又分为两种情况，一是独立样本t检验，一是配对样本t检验。</p>
<h2>t检验步骤</h2>
<p>以单总体t检验为例说明：</p>
<p>问题：难产儿出生数n=35，体重均值 $\bar x$ =3.42，S =0.40，一般婴儿出生体重μ0=3.30（大规模调查获得），问相同否？</p>
<p>解：1.建立假设、确定检验水准α</p>
<p>H0：μ = μ0 （零假设null hypothesis）</p>
<p>H1：μ ≠ μ0（备择假设alternative hypothesis）</p>
<p>双侧检验，检验水准：α=0.05</p>
<p>2.计算检验统计量</p>
<p><img src="https://gss3.bdstatic.com/7Po3dSag_xI4khGkpoWK1HF6hhy/baike/s%3D68/sign=08ccff2f9616fdfadc6cc5e6b48f3b0e/43a7d933c895d1430268e02778f082025aaf0793.jpg" alt="img"></p>
<p><img src="https://gss3.bdstatic.com/-Po3dSag_xI4khGkpoWK1HF6hhy/baike/s%3D59/sign=2714b6a0afc27d1ea1263bcd1bd57815/63d0f703918fa0ecfbd160092d9759ee3d6ddba4.jpg" alt="img"></p>
<p>3.查相应界值表，确定P值，下结论。</p>
<p>查附表1，t0.025 / 34 = 2.032,t &lt; t0.025 / 34,P &gt;0.05，按α=0.05水准，不拒绝H0，两者的差别无统计学意义</p>
<p><a href="https://baike.baidu.com/pic/t%E6%A3%80%E9%AA%8C/9910799/0/6c224f4a20a4462381b222549922720e0cf3d788?fr=lemma&amp;ct=single" target="_blank" rel="noopener"><img src="https://gss0.bdstatic.com/94o3dSag_xI4khGkpoWK1HF6hhy/baike/s%3D220/sign=3b5e0257f91986184547e8867aed2e69/6c224f4a20a4462381b222549922720e0cf3d788.jpg" alt="img"></a></p>
<h1>F检验</h1>
<p>至於F-检定，方差分析(或译变异数分析，Analysis of Variance)，它的原理大致也是上面说的，但它是透过检视变量的方差而进行的。它主要用于：均数差别的显著性检验、分离各有关因素并估计其对总变异的作用、分析因素间的交互作用、方差齐性(Equality of Variances)检验等情况。</p>
<h1>代码</h1>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> stats</span><br><span class="line"></span><br><span class="line">stats.ttest_ind(early[<span class="string">'assignment1_grade'</span>], late[<span class="string">'assignment1_grade'</span>])</span><br></pre></td></tr></table></figure></p>
<pre><code>&gt;&gt;&gt;Ttest_indResult(statistic=1.400549944897566, pvalue=0.16148283016060577)
</code></pre>
<h1>参考文献</h1>
<ol>
<li><a href="http://blog.sina.com.cn/s/blog_4ee13c2c01016div.html" target="_blank" rel="noopener">一抹新绿的博客</a></li>
<li><a href="https://baike.baidu.com/item/t%E6%A3%80%E9%AA%8C" target="_blank" rel="noopener">百度百科</a></li>
</ol>
]]></content>
      
        <categories>
            
            <category> 统计学 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 统计学 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Hexo中needmoreshare2分享微信无法加载的解决方法]]></title>
      <url>/2018/01/10/Hexo%E4%B8%ADneedmoreshare2%E5%88%86%E4%BA%AB%E5%BE%AE%E4%BF%A1%E6%97%A0%E6%B3%95%E5%8A%A0%E8%BD%BD%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95/</url>
      <content type="html"><![CDATA[<p>最近在用hexo建站时，我的一些朋友给我说了无法打开微信分享的问题。我的第一感觉是不是needmoreshare2的微信分享像disqus一样在国内被墙了，后来经过网上查阅资料发现，应该不是墙的问题，而是原来微信生成二维码的api停用了，要企业注册的话需要交钱，我们只是写个人博客的人没这个必要。</p>
<p>在needmoreshare2的github页面中issue一栏里，发现这是个普遍问题，讨论网址如下https://github.com/revir/need-more-share2/issues/4。</p>
<p>其中，有一位Aqua-Dream网友提供了一种解决方法，思路是用百度云盘生成一个指向博客的二维码，这个服务支持https站点。手机微信扫码后进入博客，点击右上角的分享即可分享到朋友圈。具体解决方法如下：</p>
<p>修改<code>themes\next\source\lib\needsharebutton\needsharebutton.js</code></p>
<p>把</p>
<p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> imgSrc = <span class="string">"https://api.qinco.me/api/qr?size=400&amp;content="</span> + <span class="built_in">encodeURIComponent</span>(myoptions.url);</span><br></pre></td></tr></table></figure></p>
<p>修改为</p>
<p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> imgSrc = <span class="string">"https://pan.baidu.com/share/qrcode?w=400&amp;h=400&amp;url="</span> + <span class="built_in">encodeURIComponent</span>(myoptions.url);</span><br></pre></td></tr></table></figure></p>
<p>即可。</p>
<p>我在想可不可以像百度分享一样在二维码旁边加一行字当做使用说明，苦于不懂JavaScript，现在还没想到合适的方法。</p>
<p>参考文献：</p>
<ol>
<li><a href="https://aqua.hk.cn/Note/Hexo%E5%8D%9A%E5%AE%A2%E8%B8%A9%E5%9D%91%E7%AC%94%E8%AE%B0/" target="_blank" rel="noopener">Aqua-Dream的博客</a></li>
</ol>
]]></content>
      
        <categories>
            
            <category> Hexo开发 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Python内置函数lambda,map,reduce,filter]]></title>
      <url>/2018/01/09/Python%E5%86%85%E7%BD%AE%E5%87%BD%E6%95%B0lambda-map-reduce-filter/</url>
      <content type="html"><![CDATA[<p>适当的使用在Python里的4个内置函数lambda, map, reduce, filter可以使程序变得非常简单易读</p>
<h1>lambda运算符</h1>
<p>函数形式：</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">lambda</span> argument_list: expression</span><br></pre></td></tr></table></figure></p>
<p>用lambda开头，然后加上所有需要在函数中使用的元素，中间用逗号隔开。元素后面加冒号，冒号后面加上函数体。</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sum = <span class="keyword">lambda</span> x, y : x + y</span><br><span class="line">sum(<span class="number">3</span>,<span class="number">4</span>)</span><br></pre></td></tr></table></figure></p>
<pre><code>7
</code></pre>
<h1>map()函数</h1>
<p>如果把map和lambda结合使用在一起，可以使函数的定义变得更加方便。</p>
<p>函数形式如下：</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">r = map(func, seq)</span><br></pre></td></tr></table></figure></p>
<p>简单来讲，map函数就是把定义的函数func运用到seq里的每一个元素上。Python2里的map函数直接返回一个list，Python3里的map函数返回一个迭代器，所以如果想要一个数列，要用list()转换迭代器结果。</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 摄氏度变华氏度</span></span><br><span class="line">C = [<span class="number">39.2</span>, <span class="number">36.5</span>, <span class="number">37.3</span>, <span class="number">38</span>, <span class="number">37.8</span>]</span><br><span class="line">F = list(map(<span class="keyword">lambda</span> x: (float(<span class="number">9</span>)/<span class="number">5</span>)*x + <span class="number">32</span>, C))</span><br><span class="line">print(F)</span><br></pre></td></tr></table></figure></p>
<pre><code>[102.56, 97.7, 99.14, 100.4, 100.03999999999999]
</code></pre>
<p>map()函数可以应用到多于一个列表上。每个列表的长度必须一样才能应用到map()函数里，过程是先把函数应用到每个列表的第一个元素上，然后每个列表的第二个元素上，以此类推。</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">a = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]</span><br><span class="line">b = [<span class="number">17</span>,<span class="number">12</span>,<span class="number">11</span>,<span class="number">10</span>]</span><br><span class="line"></span><br><span class="line">list(map(<span class="keyword">lambda</span> x,y:x+y, a,b))</span><br></pre></td></tr></table></figure></p>
<pre><code>[18, 14, 14, 14]
</code></pre>
<h1>filter()函数</h1>
<p>filter()函数提供了一种可以方便过滤掉一个数列中不需要的元素的形式。其中的function是一个描述过滤条件的函数，返回值是true或false。过程也是把function函数应用到sequence里的每一个元素上，如果function函数返回true，则这个元素会被去掉。</p>
<p>函数形式如下：</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">filter(function, sequence)</span><br></pre></td></tr></table></figure></p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fibonacci = [<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">8</span>,<span class="number">13</span>,<span class="number">21</span>,<span class="number">34</span>,<span class="number">55</span>]</span><br><span class="line">odd_numbers = list(filter(<span class="keyword">lambda</span> x: x % <span class="number">2</span>, fibonacci))</span><br><span class="line">print(odd_numbers)</span><br></pre></td></tr></table></figure></p>
<pre><code>[1, 1, 3, 5, 13, 21, 55]
</code></pre>
<h1>reduce()函数</h1>
<p>正如它的名字，reduce函数将减少数列中的元素个数。func是有两个输入，返回一个值得函数。</p>
<p>函数形式如下：</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">reduce(func, seq)</span><br></pre></td></tr></table></figure></p>
<p>如果seq= [ s1, s2, s3, ... , sn ]，使用reduce(func, seq)过程如下：</p>
<ul>
<li>第一步先将func运用到数列里前两个元素上[ func(s1, s2), s3, ... , sn ]</li>
<li>第二步将func运用到第一步的结果和数列里前三个元素上[func(func(s1, s2),s3), ... , sn ]</li>
<li>以此类推，直到最后只输出一个元素</li>
</ul>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> functools</span><br><span class="line">functools.reduce(<span class="keyword">lambda</span> x,y: x+y, [<span class="number">47</span>,<span class="number">11</span>,<span class="number">42</span>,<span class="number">13</span>])</span><br></pre></td></tr></table></figure></p>
<pre><code>113
</code></pre>
<p>另外一个例子</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> reduce</span><br><span class="line">f = <span class="keyword">lambda</span> a,b: a <span class="keyword">if</span> (a &gt; b) <span class="keyword">else</span> b</span><br><span class="line">reduce(f, [<span class="number">47</span>,<span class="number">11</span>,<span class="number">42</span>,<span class="number">102</span>,<span class="number">13</span>])</span><br></pre></td></tr></table></figure></p>
<pre><code>102
</code></pre>
<h1>参考文献</h1>
<p>[1]. Python 3 Tutorial https://www.python-course.eu/python3_lambda.php</p>
]]></content>
      
        <categories>
            
            <category> Python编程 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Python </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[集成学习Ensemble和Boosting算法]]></title>
      <url>/2018/01/08/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0Ensemble%E5%92%8CBoosting%E7%AE%97%E6%B3%95/</url>
      <content type="html"><![CDATA[<p>在完成<a href="https://haochen1.github.io/2018/01/03/Kaggle%E7%AB%9E%E8%B5%9B%EF%BC%88%E4%B8%80%EF%BC%89%E9%A2%84%E6%B5%8B%E6%B3%B0%E5%9D%A6%E5%B0%BC%E5%85%8B%E5%8F%B7%E5%B9%B8%E5%AD%98%E8%80%85%20%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5%E5%92%8C%E9%A2%84%E5%A4%84%E7%90%86/" target="_blank" rel="noopener">Kaggle竞赛（一）预测泰坦尼克号幸存者 数据导入和预处理</a>后，我们已经使用了分类里最基本的算法</p>
<ul>
<li><a href="https://haochen1.github.io/2018/01/05/%E5%86%B3%E7%AD%96%E6%A0%91Decision-Tree/" target="_blank" rel="noopener">决策树</a></li>
<li><a href="https://haochen1.github.io/2018/01/07/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97Random-Forest/" target="_blank" rel="noopener">随机森林</a></li>
</ul>
<p>我们既然已经由随机森林接触到集成学习Ensemble中的bagging算法，现在我们进一步来看集成学习中另一大类boosting算法。</p>
<h1>代码</h1>
<p>未完待补充。。</p>
<h1>原理</h1>
<p>首先给个大致的概念，同为ensemble算法，boosting和bagging的最大区别在于选择hyperspace的时候给样本加了一个权值，使得loss function尽量考虑那些分错类的样本（i.e.分错类的样本weight大）。</p>
<p>怎么做的呢？</p>
<ul>
<li>boosting重采样的不是样本，而是样本的分布，对于分类正确的样本权值低，分类错误的样本权值高（通常是边界附近的样本），最后的分类器是很多弱分类器的线性叠加（加权组合），分类器相当简单。</li>
</ul>
<p>&lt;img src=&quot;http://ww1.sinaimg.cn/mw690/b66c02d5gy1fn9twt9idsj20ht0dijsf.jpg&quot;/&gt;</p>
<p>下面我们具体来看一下两种常用的boosting算法：Adaboost和Gradient Boosting的具体原理。</p>
<h2>Adaboost</h2>
<p>Adaboost算法在分类问题中的主要特点：<strong>通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提高分类性能。</strong> AdaBoost－算法描述（伪代码）如下：</p>
<p>&lt;img src=&quot;http://ww1.sinaimg.cn/mw690/b66c02d5gy1fna0ybc11vj20he0d6dgx.jpg&quot;/&gt;</p>
<p>&lt;img src=&quot;http://ww1.sinaimg.cn/mw690/b66c02d5gy1fna0ytqioaj20hf06o3yy.jpg&quot;/&gt;</p>
<h2>Gradient Boosting</h2>
<p>提升树方法是利用加法模型与前向分布算法实现整个优化学习过程。Adaboost的指数损失和回归提升树的平方损失，在前向分布中的每一步都比较简单。但对于一般损失函数而言（比如绝对损失），每一个优化并不容易。</p>
<p>针对这一问题。Freidman提出了梯度提升（gradient boosting）算法。该算法思想：</p>
<p><strong>利用损失函数的负梯度在当前模型的值作为回归问题提升树算法中残差的近似值，拟合一个回归树。</strong></p>
<p>未完待补充。。</p>
<h1>参考文献</h1>
<p>[1]. 统计学习方法——CART, Bagging, Random Forest, Boosting http://blog.csdn.net/abcjennifer/article/details/8164315</p>
<p>[2]. CAML technology sharing platform http://www.52caml.com/head_first_ml/ml-chapter6-boosting-family/</p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
            <category> 分类问题 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 分类问题 </tag>
            
            <tag> 算法 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[随机森林Random Forest]]></title>
      <url>/2018/01/07/%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97Random-Forest/</url>
      <content type="html"><![CDATA[<h1>代码</h1>
<p>在完成<a href="https://haochen1.github.io/2018/01/03/Kaggle%E7%AB%9E%E8%B5%9B%EF%BC%88%E4%B8%80%EF%BC%89%E9%A2%84%E6%B5%8B%E6%B3%B0%E5%9D%A6%E5%B0%BC%E5%85%8B%E5%8F%B7%E5%B9%B8%E5%AD%98%E8%80%85%20%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5%E5%92%8C%E9%A2%84%E5%A4%84%E7%90%86/" target="_blank" rel="noopener">Kaggle竞赛（一）预测泰坦尼克号幸存者 数据导入和预处理</a>后，我们首先使用了分类里最基本的算法<a href="https://haochen1.github.io/2018/01/05/%E5%86%B3%E7%AD%96%E6%A0%91Decision-Tree/" target="_blank" rel="noopener">决策树</a>来完成第一个分类模型，现在来看下一个模型随机森林Random Forest。</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 载入准确度检验函数</span></span><br><span class="line">shuffle_validator = cross_validation.ShuffleSplit(len(X), n_iter=<span class="number">20</span>, test_size=<span class="number">0.2</span>, random_state=<span class="number">0</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_classifier</span><span class="params">(clf)</span>:</span></span><br><span class="line">    scores = cross_validation.cross_val_score(clf, X, y, cv=shuffle_validator)</span><br><span class="line">    print(<span class="string">"Accuracy: %0.4f (+/- %0.2f)"</span> % (scores.mean(), scores.std()))</span><br></pre></td></tr></table></figure></p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sklearn.ensemble <span class="keyword">as</span> ske</span><br><span class="line"><span class="comment"># Random forest </span></span><br><span class="line">clf_rf = ske.RandomForestClassifier(n_estimators=<span class="number">100</span>)</span><br><span class="line">clf_rf.fit (X_train, y_train)</span><br><span class="line">test_classifier(clf_rf)</span><br></pre></td></tr></table></figure></p>
<pre><code>Accuracy: 0.8101 (+/- 0.03)
</code></pre>
<p>和上篇类似，如果只对代码感兴趣，可以在此停住。下面会是一些原理部分。</p>
<h1>原理</h1>
<p>classification tree非常简单，但是经常会有noisy classifiers. 于是引入ensemble classifiers: bagging, random forest（决策树和bagging的结合）, 和boosting。</p>
<p>一般的，用团体学习会比不用的算法准确度高， Boosting &gt; Bagging &gt; Classification tree(single tree)。</p>
<h2>Bagging装袋算法</h2>
<p>Bagging算法 （英语：Bootstrap aggregating，引导聚集算法），又称装袋算法，是机器学习领域的一种团体学习ensemble算法。最初由Leo Breiman于1994年提出。 Bagging算法可与其他分类、回归算法结合，提高其准确率、稳定性的同时，通过降低结果的方差，避免过拟合的发生。</p>
<p>Bagging的策略：</p>
<ul>
<li>从样本集中用Bootstrap采样选出n个样本</li>
<li>在所有属性上，对这n个样本建立分类器（CART or SVM or ...）</li>
<li>重复以上两步m次，i.e.build m个分类器（CART or SVM or ...）</li>
<li>将数据放在这m个分类器上跑，最后vote看到底分到哪一类</li>
</ul>
<p>下图是Bagging的选择策略，每次从N个数据中采样n次得到n个数据的一个bag，总共选择B次得到B个bags，也就是B个bootstrap samples.</p>
<p>&lt;img src=&quot;http://ww1.sinaimg.cn/mw690/b66c02d5gy1fn8p69xdj9j20j10dmq3s.jpg&quot;/&gt;</p>
<h2>Random forest随机森林</h2>
<p>随机森林在bagging基础上做了修改。</p>
<ul>
<li>从样本集中用Bootstrap采样选出n个样本，预建立CART</li>
</ul>
<ul>
<li>在树的每个节点上，从所有属性中随机选择k个属性，选择出一个最佳分割属性作为节点</li>
</ul>
<ul>
<li>重复以上两步m次，i.e.build m棵CART</li>
</ul>
<ul>
<li>这m个CART形成Random Forest</li>
</ul>
<p>这里的random就是指</p>
<ol>
<li>Bootstrap中的随机选择子样本</li>
<li>Random subspace的算法从属性集中随机选择k个属性，每个树节点分裂时，从这随机的k个属性，选择最优的</li>
</ol>
<h1>参考文献</h1>
<p>[1].统计学习方法——CART, Bagging, Random Forest, Boosting http://blog.csdn.net/abcjennifer/article/details/8164315</p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
            <category> 分类问题 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 分类问题 </tag>
            
            <tag> 算法 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[决策树Decision Tree]]></title>
      <url>/2018/01/05/%E5%86%B3%E7%AD%96%E6%A0%91Decision-Tree/</url>
      <content type="html"><![CDATA[<h1>代码</h1>
<p>继上篇博客<a href="https://haochen1.github.io/2018/01/03/Kaggle%E7%AB%9E%E8%B5%9B%EF%BC%88%E4%B8%80%EF%BC%89%E9%A2%84%E6%B5%8B%E6%B3%B0%E5%9D%A6%E5%B0%BC%E5%85%8B%E5%8F%B7%E5%B9%B8%E5%AD%98%E8%80%85%20%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5%E5%92%8C%E9%A2%84%E5%A4%84%E7%90%86/" target="_blank" rel="noopener">Kaggle竞赛（一）预测泰坦尼克号幸存者 数据导入和预处理</a>。现在正式进入机器学习的算法部分，我想介绍的第一个是决策树Decision Tree。</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Decision Tree</span></span><br><span class="line">score=[]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">20</span>):</span><br><span class="line">    clf_dt = tree.DecisionTreeClassifier(max_depth=i)</span><br><span class="line">    clf_dt.fit (X_train, y_train)</span><br><span class="line">    score.append(clf_dt.score (X_test, y_test))</span><br><span class="line">plt.bar(range(<span class="number">1</span>,<span class="number">20</span>),score)</span><br></pre></td></tr></table></figure></p>
<p>&lt;img src=&quot;http://ww1.sinaimg.cn/large/b66c02d5gy1fn5fnhiuqtj20ah070jr8.jpg&quot;/&gt;</p>
<p>我们看出来3层是最好的深度。然后我们用cross-validation来算20次随机分配后的准确度。</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">clf_dt = tree.DecisionTreeClassifier(max_depth=<span class="number">3</span>)</span><br><span class="line">shuffle_validator = cross_validation.ShuffleSplit(len(X), n_iter=<span class="number">20</span>, test_size=<span class="number">0.2</span>, random_state=<span class="number">0</span>)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test_classifier</span><span class="params">(clf)</span>:</span></span><br><span class="line">    scores = cross_validation.cross_val_score(clf, X, y, cv=shuffle_validator)</span><br><span class="line">    print(<span class="string">"Accuracy: %0.4f (+/- %0.2f)"</span> % (scores.mean(), scores.std()))</span><br><span class="line">test_classifier(clf_dt)</span><br></pre></td></tr></table></figure></p>
<pre><code>Accuracy: 0.8149 (+/- 0.03)
</code></pre>
<p>最后，3层决策树的准确度大概是81.49%。</p>
<p>如果已有基础，只想知道怎么写代码，怎么将它应用到泰坦尼克数据上，可以只看到这。</p>
<h1>原理</h1>
<ul>
<li>优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特
征数据。</li>
<li>缺点：可能会产生过度匹配问题。</li>
<li>适用数据类型：数值型和标称型。</li>
</ul>
<h2>决策树的构造</h2>
<p>在构造决策树时，我们需要解决的第一个问题就是，当前数据集上哪个特征在划分数据分类时起决定性作用。为了找到决定性的特征，划分出最好的结果，我们必须评估每个特征。完成测试之后，原始数据集就被划分为几个数据子集。</p>
<h3>ID3算法</h3>
<p>我们用ID3算法划分数据集，该算法处理如何划分数据集，何时停止划分数据集，进一步的信息可以参见<a href="http://en.wikipedia.org/wiki/ID3_algorithm" target="_blank" rel="noopener">wikipedia</a>。每次划分数据集时我们只选取一个特征属性，如果训练集中存在20个特征，第一次我们选择哪个特征作为划分的参考属性呢？</p>
<p>下面数据包含5个海洋动物，特征包括：不浮出水面是否可以生存，以及是否有脚蹼。我们可以将这些动物分成两类：鱼类和非鱼类。现在我们想要决定依据第一个特征还是第二个特征划分数据。在回答这个问题之前，我们必须采用量化的方法判断如何划分数据。下一小节将详细讨论这个问题。</p>
<table>
<thead>
<tr>
<th></th>
<th>不浮出水面是否可以生存No surfacing?</th>
<th>是否有脚蹼Flippers?</th>
<th style="text-align:left">属于鱼类Fish?</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>是</td>
<td>是</td>
<td style="text-align:left">是</td>
</tr>
<tr>
<td>2</td>
<td>是</td>
<td>是</td>
<td style="text-align:left">是</td>
</tr>
<tr>
<td>3</td>
<td>是</td>
<td>否</td>
<td style="text-align:left">否</td>
</tr>
<tr>
<td>4</td>
<td>否</td>
<td>是</td>
<td style="text-align:left">否</td>
</tr>
<tr>
<td>5</td>
<td>否</td>
<td>是</td>
<td style="text-align:left">否</td>
</tr>
</tbody>
</table>
<h4>信息增益</h4>
<p>熵定义为信息的期望值。一个系统越是有序，信息熵就越低；反之，一个系统越是混乱，信息熵就越高。在明晰这个概念之前，我们必须知道信息的定义。如果待分类的事务可能划分在多个分类之中，则符号xi的信息定义为
$$
l(x_i)=-log_2 p(x_i)
$$
其中是 $ p(x_i) $选择该分类的概率。</p>
<p>为了计算熵，我们需要计算所有类别所有可能值包含的信息期望值，通过下面的公式得到：
$$
H=-\sum^n_{i=1}p(x_i)log_2p(x_i)
$$
其中n 是分类的数目。</p>
<p>ID3(Iterative Dichotomiser 3 迭代二叉树3代)划分特征使用的就是<strong>信息增益IG（Information Gain）</strong>。一个属性的信息增益越大，表明属性对样本的熵减少的能力就更强，该属性使得数据所属类别的不确定性变为确定性的能力越强。信息增益在统计学中称为互信息，互信息是条件概率与后验概率的比值，化简之后就可以得到信息增益。所以说互信息其实就是信息增益。计算方法【互信息=熵-条件熵】。</p>
<p>首先计算特征A对数据集D的经验<strong>条件熵</strong>H(D|A),在数学上就是条件概率分布（Condition Probability）.
$$
H(D|A)=\sum_j \frac{D_j}{D} \times H(D_j), \frac{D_j}{D}是第j个分区的权重
$$
引入条件熵，在信息论中主要是为了消除结果的不确定性。然后计算信息增益
$$
Gain(A)=H(D)-H(D|A)
$$
在上面的例子中，
$$
H(D)=-\frac{2}{5} log_2\frac{2}{5}-\frac{3}{5} log_2\frac{3}{5}=0.971位
$$</p>
<p>$$
H(D|No\ surfacing)=\frac{3}{5}(-\frac{2}{3}log_2\frac{2}{3}-\frac{1}{3}log_2\frac{1}{3})+\frac{2}{5}(-\frac{2}{2}log_2\frac{2}{2}-\frac{0}{2}log_2\frac{0}{2})=0.551位
$$</p>
<p>$$
H(D|Flippers)=\frac{4}{5}(-\frac{2}{4}log_2\frac{2}{4}-\frac{2}{4}log_2\frac{2}{4})+\frac{1}{5}(-\frac{0}{1}log_2\frac{0}{1}-\frac{1}{1}log_2\frac{1}{1})=0.8位
$$</p>
<p>$$
Gain(No \  Surfacing)=0.971-0.551=0.42位
$$</p>
<p>$$
Gain(Flippers)=0.971-0.8=0.171位
$$</p>
<p>由于No Surfacing在属性中具有最高的信息增益，所以它被选作根节点的分裂特征。下面再进行递归计算信息增益，在此就不展示了。</p>
<h3>C4.5算法</h3>
<p>与ID3算法类似，我们还可以用C4.5（使用增益率）</p>
<p>C4.5算法有如下优点：<strong>产生的分类规则易于理解，准确率较高</strong>。其缺点是：<strong>在构造树的过程中，需要对数据集进行多次的顺序扫描和排序，因而导致算法的低效</strong>。另外，C4.5只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时程序无法运行。</p>
<p>另外，无论是ID3还是C4.5最好在小数据集上使用，决策树分类一般只试用于小数据。当属性取值很多时最好选择C4.5算法，ID3得出的效果会非常差，因为使用信息增益划分时它倾向于取值多的属性。</p>
<p>首先要计算<strong>分裂信息</strong>：
$$
Split_H(D|A)=-\sum \frac{|D_j|}{D} \times log_2(\frac{|D_j|}{D})
$$
再计算<strong>信息增益率</strong>：
$$
IG Rate(A)=\frac{Gain(A)}{Split_H(D|A)}
$$
选择最大增益率的特征作为分裂特征。</p>
<h3>CART算法</h3>
<p>在CART算法，我们将用到基尼指数，sklearn中决策树和随机森林中默认的属性划分标准也是它。Gini index划分是二元的，<strong>它度量的是数据分区或训练元组集D的不纯度，表示的是一个随机选中的样本在子集中被分错的可能性</strong>。
$$
Gini(D)=1-\sum p_i^2，其中，p_i是D中元组数以C_i类的概率，对m个类计算和
$$
Gini指数越大，不纯度越大，越不容易区分。假设A有v个不同的值出现在特征D中，它的二元划分有2v−22v−2种（除去自己和空集）。当考虑二元划分裂时，计算每个结果分区的不纯度加权和。比如A有两个值，则特征D被划分成D1和D2,这时Gini指数为：
$$
Gini_A(D)=\frac{D_1}{D}Gini(D_1)+\frac{D_2}{D}Gini(D_2)
$$
对于每个属性，考虑每种可能的二元划分，对于离散值属性，<strong>选择该属性产生最小Gini指数的自己作为它的分裂信息</strong>。</p>
<h1>参考文献</h1>
<ol>
<li>刘帝伟的博客 http://www.csuldw.com/2015/05/08/2015-05-08-decision%20tree</li>
<li>Machine Learning in action https://www.manning.com/books/machine-learning-in-action</li>
</ol>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
            <category> 分类问题 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> 分类问题 </tag>
            
            <tag> 算法 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Kaggle竞赛（一）预测泰坦尼克号幸存者 数据导入和预处理]]></title>
      <url>/2018/01/03/Kaggle%E7%AB%9E%E8%B5%9B%EF%BC%88%E4%B8%80%EF%BC%89%E9%A2%84%E6%B5%8B%E6%B3%B0%E5%9D%A6%E5%B0%BC%E5%85%8B%E5%8F%B7%E5%B9%B8%E5%AD%98%E8%80%85%20%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5%E5%92%8C%E9%A2%84%E5%A4%84%E7%90%86/</url>
      <content type="html"><![CDATA[<p>在注册完Kaggle账号后，我们就可以来试一试Kaggle上最简单的分类问题，利用当年泰坦尼克号乘客的个人信息来预测他们其中的幸存者。</p>
<h1>下载数据集</h1>
<p>进入<a href="https://www.kaggle.com/c/titanic/data" target="_blank" rel="noopener">Kaggle Titanic</a>页面下载数据集，我们可以看到这里有三个csv文件
&lt;img src=&quot;http://ww1.sinaimg.cn/mw690/b66c02d5gy1fn34luyh1pj20py04gjrs.jpg&quot;/&gt;</p>
<ul>
<li>train.csv是我们训练用数据，我们将使用这些数据建模</li>
<li>test.csv是测试用数据，我们将用这些数据检验模型的准确度</li>
<li>gender_submossion.csv提供给我们知道最后应该提交什么样子的文件</li>
</ul>
<h1>导入数据集</h1>
<p>我个人习惯用<a href="http://jupyter.org/" target="_blank" rel="noopener">Jupyter Notebook</a>，它可以直接生成markdown模式。可以直接安装<a href="https://anaconda.org/" target="_blank" rel="noopener">anaconda</a>，里面就包括了Jupyter Notebook。我们要做的是首先打开Jupyter Notebook，然后在数据集同个目录里新建一个Python notebook。然后正式开始我们的代码：</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入几个基本库</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure></p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df=pd.read_csv(<span class="string">'train.csv'</span>)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure></p>
<p>&lt;img src=&quot;http://ww1.sinaimg.cn/mw690/b66c02d5gy1fn49fc552dj20r004h3zk.jpg&quot;/&gt;</p>
<p>我们看到这个数据集包括：</p>
<ol>
<li>PassengerId，乘客的序号，从第一位开始到最后一位乘客</li>
<li>Survived，0表示该乘客死亡，1表示生还</li>
<li>Pclass，从1到3分别表示该乘客搭乘一等舱、二等舱、三等舱</li>
<li>Name，乘客的姓名，格式是：名，称谓.姓氏</li>
<li>Sex，乘客的性别，female女，male男</li>
<li>Age，乘客的年龄</li>
<li>SibSp，该乘客的兄弟姐妹和配偶也在船上的数量</li>
<li>Parch，该乘客的父母和子女也在船上的数量</li>
<li>Ticket，票号</li>
<li>Fare，票价</li>
<li>Cabin，客舱号</li>
<li>Embarked，登船港口，C = Cherbourg, Q = Queenstown, S = Southampton</li>
</ol>
<h1>几个基本的分析</h1>
<h2>训练集统计</h2>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.count()</span><br></pre></td></tr></table></figure></p>
<pre><code>PassengerId    891
Survived       891
Pclass         891
Name           891
Sex            891
Age            714
SibSp          891
Parch          891
Ticket         891
Fare           891
Cabin          204
Embarked       889
dtype: int64
</code></pre>
<p>我们可以看到在训练集train.csv里一共891名乘客，其中Age，Cabin，Embarked不全，为了模型的准确度，我们最好想办法补全它们。</p>
<h2>总体生还率</h2>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df[<span class="string">'Survived'</span>].mean()</span><br></pre></td></tr></table></figure></p>
<pre><code>0.3838383838383838
</code></pre>
<p>总体生还率是38.38%</p>
<h2>按照客舱等级分组</h2>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">calss_grouping=df.groupby(<span class="string">'Pclass'</span>).mean()</span><br><span class="line">calss_grouping</span><br></pre></td></tr></table></figure></p>
<p>&lt;img src=&quot;http://ww1.sinaimg.cn/mw690/b66c02d5gy1fn49fz3neqj20d003tdga.jpg&quot;/&gt;</p>
<p>我们看到客舱越高级，生还率越高，有钱真好...</p>
<h2>按客舱等级加性别分组</h2>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">class_sex_grouping = df.groupby([<span class="string">'Pclass'</span>,<span class="string">'Sex'</span>]).mean()</span><br><span class="line">class_sex_grouping</span><br></pre></td></tr></table></figure></p>
<p>&lt;img src=&quot;http://ww1.sinaimg.cn/mw690/b66c02d5gy1fn49intgf2j20ew0630to.jpg&quot;/&gt;</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">class_sex_grouping[<span class="string">'Survived'</span>].plot.bar()</span><br></pre></td></tr></table></figure></p>
<p>&lt;img src=&quot;http://ww1.sinaimg.cn/mw690/b66c02d5gy1fn48k5bejzj20af08njr9.jpg&quot;/&gt;</p>
<p>每个客舱等级里女性生还几率较大，还是比较绅士的。</p>
<h2>按年龄层分组</h2>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">group_by_age = pd.cut(df[<span class="string">"Age"</span>], np.arange(<span class="number">0</span>, <span class="number">90</span>, <span class="number">10</span>))</span><br><span class="line">age_grouping = df.groupby(group_by_age).mean()</span><br><span class="line">age_grouping[<span class="string">'Survived'</span>].plot.bar()</span><br></pre></td></tr></table></figure></p>
<p>&lt;img src=&quot;http://ww1.sinaimg.cn/mw690/b66c02d5gy1fn48m7aqd0j20af087a9x.jpg&quot;/&gt;</p>
<p>貌似年轻的生还几率比较大。</p>
<h2>测试集统计</h2>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df_test=pd.read_csv(<span class="string">'test.csv'</span>)</span><br><span class="line">df_test.count()</span><br></pre></td></tr></table></figure></p>
<pre><code>PassengerId    418
Pclass         418
Name           418
Sex            418
Age            332
SibSp          418
Parch          418
Ticket         418
Fare           417
Cabin           91
Embarked       418
dtype: int64
</code></pre>
<p>测试集的数据缺失发生在Age，Fare和Cabin上，和训练集并不一样，这点要注意。</p>
<h1>处理缺失数据</h1>
<h2>处理年龄</h2>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 不同称谓的年龄中位数</span></span><br><span class="line">title_age_grouping=df[[<span class="string">'Title'</span>,<span class="string">'Age'</span>]].groupby(<span class="string">'Title'</span>).median()</span><br><span class="line">title_age_grouping.plot.bar()</span><br></pre></td></tr></table></figure></p>
<p>&lt;img src=&quot;http://ww1.sinaimg.cn/mw690/b66c02d5gy1fn5dxez5kjj20ac08xjrc.jpg&quot;/&gt;</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 补全年龄的函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fill_ages</span><span class="params">(df,title_age_grouping)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(df.index)):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(title_age_grouping.index)):</span><br><span class="line">            <span class="keyword">if</span> df.loc[i,<span class="string">'Title'</span>] == title_age_grouping.index[j] <span class="keyword">and</span> np.isnan(df.loc[i,<span class="string">'Age'</span>]):</span><br><span class="line">                df.loc[i,<span class="string">'Age'</span>]=title_age_grouping.Age[j]</span><br></pre></td></tr></table></figure></p>
<h2>处理票价</h2>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 不同客舱等级和称谓的票价中位数</span></span><br><span class="line">class_title_fare_grouping=df[[<span class="string">'Pclass'</span>,<span class="string">'Title'</span>,<span class="string">'Fare'</span>]].groupby([<span class="string">'Pclass'</span>,<span class="string">'Title'</span>]).median()</span><br><span class="line">class_title_fare_grouping.plot.bar()</span><br></pre></td></tr></table></figure></p>
<p>&lt;img src=&quot;http://ww1.sinaimg.cn/mw690/b66c02d5gy1fn5dytzarmj20ai09ijrf.jpg&quot;/&gt;</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 补全票价的函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fill_fares</span><span class="params">(df,class_title_fare_grouping)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(df.index)):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(class_title_fare_grouping.index)):</span><br><span class="line">            <span class="keyword">if</span> (df.loc[i,<span class="string">'Pclass'</span>], df.loc[i,<span class="string">'Title'</span>])== class_title_fare_grouping.index[j] <span class="keyword">and</span> np.isnan(df.loc[i,<span class="string">'Fare'</span>]):</span><br><span class="line">                df.loc[i,<span class="string">'Fare'</span>]=class_title_fare_grouping.Fare[j]</span><br></pre></td></tr></table></figure></p>
<h1>最后总体处理</h1>
<ol>
<li>把上面的补全年龄和票价的函数用在数据集上，</li>
<li>先将'Cabin'列drop掉，因为缺少很多数据，并且我没想到怎么样补全客舱数据，</li>
<li>将缺少数据的行用df.dropna()清理掉，</li>
<li>再将无用的行'PassengerId','Name','Ticket'列drop掉，</li>
<li>编码</li>
</ol>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_titanic_df</span><span class="params">(df,title_age_grouping,class_title_fare_grouping)</span>:</span></span><br><span class="line">    <span class="comment">#copy</span></span><br><span class="line">    processed_df = df.copy()</span><br><span class="line">    <span class="comment">#get titles</span></span><br><span class="line">    get_titles(processed_df)</span><br><span class="line">    <span class="comment">#fill missing ages</span></span><br><span class="line">    fill_ages(processed_df,title_age_grouping)</span><br><span class="line">    <span class="comment">#fill missing fares</span></span><br><span class="line">    fill_fares(processed_df,class_title_fare_grouping)</span><br><span class="line">    <span class="comment">#drop cabin variable</span></span><br><span class="line">    processed_df = processed_df.drop([<span class="string">'Cabin'</span>], axis=<span class="number">1</span>)</span><br><span class="line">    <span class="comment">#drop na</span></span><br><span class="line">    processed_df = processed_df.dropna()</span><br><span class="line">    <span class="comment">#label encoder</span></span><br><span class="line">    le = preprocessing.LabelEncoder()</span><br><span class="line">    processed_df.Sex = le.fit_transform(processed_df.Sex)</span><br><span class="line">    processed_df.Embarked = le.fit_transform(processed_df.Embarked)</span><br><span class="line">    processed_df.Title = le.fit_transform(processed_df.Title)</span><br><span class="line">    processed_df = processed_df.drop([<span class="string">'PassengerId'</span>,<span class="string">'Name'</span>,<span class="string">'Ticket'</span>],axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> processed_df</span><br></pre></td></tr></table></figure></p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">processed_df = preprocess_titanic_df(df,title_age_grouping,class_title_fare_grouping)</span><br><span class="line">processed_df.count()</span><br></pre></td></tr></table></figure></p>
<pre><code>Survived    889
Pclass      889
Sex         889
Age         889
SibSp       889
Parch       889
Fare        889
Embarked    889
Title       889
dtype: int64
</code></pre>
<p>最后将训练集分成80%的训练集train和20%的验证集test，注意区分这里的验证集test是已知结果的，用来验证比较各种模型。最后我们还有test.csv是未知结果的，用来最后验证准确度。</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> model_selection</span><br><span class="line"></span><br><span class="line">X = processed_df.drop([<span class="string">'Survived'</span>], axis=<span class="number">1</span>).values</span><br><span class="line">y = processed_df[<span class="string">'Survived'</span>].values</span><br><span class="line">                </span><br><span class="line">X_train, X_test, y_train, y_test = model_selection.train_test_split(X,y,test_size=<span class="number">0.2</span>)  </span><br><span class="line"></span><br><span class="line">print(<span class="string">'X_train'</span>,X_train.shape)</span><br><span class="line">print(<span class="string">'y_train'</span>,y_train.shape)</span><br><span class="line">print(<span class="string">'X_test'</span>,X_test.shape)</span><br><span class="line">print(<span class="string">'y_test'</span>,y_test.shape)</span><br></pre></td></tr></table></figure></p>
<pre><code>X_train (711, 8)
y_train (711,)
X_test (178, 8)
y_test (178,)
</code></pre>
<p>现在数据预处理已经完成，下一步正式进入机器学习分类算法。</p>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
            <category> 分类问题 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Kaggle </tag>
            
            <tag> 分类问题 </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Kaggle平台]]></title>
      <url>/2018/01/02/Kaggle%E5%B9%B3%E5%8F%B0/</url>
      <content type="html"><![CDATA[<h1>为什么要先介绍<a href="https://www.kaggle.com/" target="_blank" rel="noopener">Kaggle</a></h1>
<p>前段时间一直在找工作，在面试过程中经常被问到“你是否有Kaggle排名？”，我说“没有”，然后就被堵“没有，怎么证明你比别人更懂数据科学？”。。。</p>
<p>还是学生时，没有太注意积累这些排名，现在补估计还来得及。</p>
<p>另外，我发现传统的数据科学学习模式，先介绍一大堆的理论和算法，学到后面前面的就忘了，所以我想着是不是可以先介绍一个具体案例，然后把分类或回归的算法一个个用在上面。</p>
<h1>所以什么是<a href="https://www.kaggle.com/" target="_blank" rel="noopener">Kaggle</a></h1>
<p>就像国内阿里的天池大数据平台，<a href="https://www.kaggle.com/" target="_blank" rel="noopener">Kaggle</a>是国外领先的数据科学、机器学习竞赛平台，最近已经被Google收购。我们可以在上面下载数据集，应用我们机器学习的算法来找出结果，最后提交结果获得排名，如果你参加的是有报酬的竞赛，进入前几名还能获得报酬。最后你还能跟别人组队参赛，或者在网站上的Kernel里分享你的代码，和别人进行讨论。</p>
<h1>如何注册<a href="https://www.kaggle.com/" target="_blank" rel="noopener">Kaggle</a></h1>
<ol>
<li>进入<a href="https://www.kaggle.com/" target="_blank" rel="noopener">Kaggle</a>，网址是https://www.kaggle.com/</li>
<li>点击下面的Create an account</li>
<li>在弹出的窗口中，可以选择用Facebook、Google或Yahoo的账号注册，也可以点击Manually create a new account选择手动录入信息</li>
<li>输入一切信息后，你就有一个Kaggle账号了，以后kaggle.com/username（你刚刚输入的用户名）就是你的kaggle主页了</li>
<li>登录账号后，最上方的<a href="https://www.kaggle.com/competitions" target="_blank" rel="noopener">Competitions</a>就是我们参加各种竞赛的地方；<a href="https://www.kaggle.com/datasets" target="_blank" rel="noopener">Datasets</a>是下载所有数据集的页面，其中包括不是竞赛的数据集；<a href="https://www.kaggle.com/kernels" target="_blank" rel="noopener">Kernels</a>是分享代码的页面，支持markdown。</li>
<li>等我们参加了竞赛有排名后，我们就能点击右上角的头像进入My Profile来看我们的排名。像我的页面就是https://www.kaggle.com/haochen1</li>
</ol>
]]></content>
      
        <categories>
            
            <category> 机器学习 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Kaggle </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Hello World]]></title>
      <url>/2017/12/30/hello-world/</url>
      <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2>Quick Start</h2>
<h3>Create a new post</h3>
<p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure></p>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3>Run server</h3>
<p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure></p>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3>Generate static files</h3>
<p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure></p>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3>Deploy to remote sites</h3>
<p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure></p>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>
]]></content>
      
        <categories>
            
            <category> Hexo开发 </category>
            
        </categories>
        
        
        <tags>
            
            <tag> hexo </tag>
            
        </tags>
        
    </entry>
    
  
  
</search>
