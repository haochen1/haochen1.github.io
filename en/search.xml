<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title><![CDATA[Re-Id paper resumes]]></title>
      <url>/en/2018/09/22/Re-Id%20Papers/</url>
      <content type="html"><![CDATA[<h1>Re-Id Papers</h1>
<h3>1.Unsupervised Person Re-identification by Deep Learning Tracklet Association [ECCV 2018]</h3>
<ul>
<li>
<p><strong>Keywords:</strong> Unsupervised, automatically generated person tracklet</p>
</li>
<li>
<p><strong>Main idea:</strong> Tracklet Association Unsupervised Deep Learning (TAUDL): maximize cross-camera tracklet similarity and within-camera tracklet dissimilarity in an end-to-end deep learning framework.</p>
</li>
</ul>
<h3>2. Improving Person Re-identification by Attribute and Identity Learning [arVix 2017]</h3>
<ul>
<li><strong>Keywords:</strong> Attributes, multi-task</li>
<li><strong>Main idea:</strong> The author proposed a new attribute-person recognition (APR) network which combines the ID and attribute classification losses. 27 attributes for Market-1501 and 23 attributes for Duke are labeled.</li>
</ul>
<h3>3. Person Re-identification using CNN Features Learned from Combination of Attributes [ICPR 2016]</h3>
<ul>
<li><strong>Keywords:</strong> Fine-tuning, attributes</li>
<li><strong>Main idea:</strong> The author firstly conducts a fine-tuning (pre-trained AlexNet on ImageNet) on Pedestrian Attribute dataset (PETA), then applies metric learning (Cross View Quadratic Discriminant Analysis XQDA) on the target dataset.</li>
</ul>
<h3>4. Diversity Regularized Spatiotemporal Attention for Video-based Person-identification [CVPR 2018]</h3>
<ul>
<li><strong>Keywords:</strong> Video-based re-id, attention model</li>
<li><strong>Main idea:</strong> Spatial and temporal attentions are combined into a spatiotemporal attention models. Then a penalization term is used to regularize multiple redundant attentions.</li>
</ul>
<h3>5.  Deep Association Learning for Unsupervised video Re-identification [BMVC 2018]</h3>
<ul>
<li><strong>Keywords:</strong> Unsupervised</li>
<li><strong>Main idea:</strong> Deep Association Learning (DAL): (1) local space-time consistency within each tracklet from the same camera view and (2) global cyclic ranking consistency between tracklet from disjoint camera views.</li>
</ul>
<h3>6. Unsupervised data association for Metric Learning in the context of Multi-shot Person Re-identification [AVSS 2016]</h3>
<ul>
<li><strong>Keywords:</strong> Metric learning</li>
<li><strong>Main idea:</strong> The main idea is to automatically label data with signatures represented by a set of multi-modal feature distributions (GMMs) for metric learning (Mahalanobis distance Learning) using KISSME algorithm.</li>
</ul>
<h3>7. Multi-shot Person Re-identification using Part Appearance Mixture [WACV 2017]</h3>
<ul>
<li><strong>Keywords:</strong> Metric learning</li>
<li><strong>Main idea:</strong>  Like 6, firstly represent a person's appearance by a signature model, then compare distance between signatures.</li>
</ul>
<h3>8. Cross domain Residual Transfer Learning for Person Re-identification [WACV 2019]</h3>
<ul>
<li><strong>Keywords:</strong> Residual transfer learning</li>
<li><strong>Main idea:</strong> Turn fine tuning into 4 stages Residual Transfer Learning(RTL). From a pretrained VGG16, (a) Only train last layers like fine tuning, (b) add and train residual blocks, (c) train residual units and last layers ,(d) optional: retrain all layers.</li>
</ul>
<table>
<thead>
<tr>
<th></th>
<th>CUHK03</th>
<th></th>
<th>Market-1501</th>
<th></th>
<th>DukeMTMC</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>R1</td>
<td>mAP</td>
<td>R1</td>
<td>mAP</td>
<td>R1</td>
<td>mAP</td>
</tr>
<tr>
<td>1.TAUDL</td>
<td>44.7</td>
<td>31.2</td>
<td>63.7</td>
<td>41.2</td>
<td>61.7</td>
<td>43.5</td>
</tr>
<tr>
<td>2.APR</td>
<td></td>
<td></td>
<td>84.29</td>
<td>64.67</td>
<td>70.69</td>
<td>51.88</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th></th>
<th>PRID2011</th>
<th></th>
<th>iLIDS-VID</th>
<th></th>
<th>MARS</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>R1</td>
<td>R5</td>
<td>R1</td>
<td>R5</td>
<td>R1</td>
<td>R5</td>
<td>mAP</td>
</tr>
<tr>
<td>1.TAUDL</td>
<td>49.4</td>
<td>78.7</td>
<td>26.7</td>
<td>51.3</td>
<td>43.8</td>
<td>59.9</td>
<td>29.1</td>
</tr>
<tr>
<td>4.SpaAtn+Q+TemAtn+Ind</td>
<td>93.2</td>
<td></td>
<td>80.2</td>
<td></td>
<td>82.3</td>
<td></td>
<td></td>
</tr>
<tr>
<td>5.DAL(ResNet50)</td>
<td>85.3</td>
<td>97</td>
<td>56.9</td>
<td>80.6</td>
<td>46.8</td>
<td>63.9</td>
<td>21.4</td>
</tr>
<tr>
<td>6.MCM+KISSME</td>
<td>64.3</td>
<td>86.1</td>
<td>40.3</td>
<td>69.9</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>7.PAM+LOMO+KISSME</td>
<td>92.5</td>
<td></td>
<td>79.5</td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>8.RTL+XQDA</td>
<td>92.1</td>
<td></td>
<td>78.7</td>
<td></td>
<td>67.9</td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th></th>
<th>VIPeR</th>
<th>CUHK01</th>
<th>PRID450S</th>
<th>GRID</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>R1</td>
<td>R1</td>
<td>R1</td>
<td>R1</td>
</tr>
<tr>
<td>3.FT-CNN+XQDA</td>
<td>42.5</td>
<td>46.8</td>
<td>58.2</td>
<td>25.2</td>
</tr>
<tr>
<td>3.FT-CNN+LOMO+XQDA</td>
<td>52.1</td>
<td>62.3</td>
<td>71.5</td>
<td>29.1</td>
</tr>
</tbody>
</table>
]]></content>
      
        <categories>
            
            <category> Re-Id </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Computer Vision </tag>
            
            <tag> Re-Id resources </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Some scentists on Re-Id to follow]]></title>
      <url>/en/2018/09/22/Re-Id%20People/</url>
      <content type="html"><![CDATA[<h1>Re-Id Scientists</h1>
<ol>
<li>
<p><strong>Shaogang Gong</strong></p>
<ul>
<li>Professor at Queen Mary university of London</li>
<li>Publications: <a href="http://www.eecs.qmul.ac.uk/~sgg/publication.html" target="_blank" rel="noopener">http://www.eecs.qmul.ac.uk/~sgg/publication.html</a></li>
</ul>
</li>
<li>
<p><strong>Tao Xiang</strong></p>
<ul>
<li>Professor at Queen Mary university of London</li>
<li>Publications: <a href="http://www.eecs.qmul.ac.uk/~txiang/publications.html" target="_blank" rel="noopener">http://www.eecs.qmul.ac.uk/~txiang/publications.html</a></li>
</ul>
</li>
<li>
<p><strong>Xiaogang Wang</strong></p>
<ul>
<li>Associate professor at Chinese University of Hong Kong</li>
<li>Publications: <a href="http://www.ee.cuhk.edu.hk/~xgwang/publications_topic.html" target="_blank" rel="noopener">http://www.ee.cuhk.edu.hk/~xgwang/publications_topic.html</a></li>
</ul>
</li>
<li>
<p><strong>Hongsheng Li</strong></p>
<ul>
<li>Assistant Professor at Chinese University of Hong Kong</li>
<li>Publications: <a href="http://www.ee.cuhk.edu.hk/~hsli/" target="_blank" rel="noopener">http://www.ee.cuhk.edu.hk/~hsli/</a></li>
</ul>
</li>
<li>
<p><strong>Liang Zheng</strong></p>
<ul>
<li>Assistant Professor at <a href="https://sutd.edu.sg/" target="_blank" rel="noopener">Singapore University of Technology and Design</a></li>
<li>Publications: <a href="">http://www.liangzheng.com.cn/Publication.html</a></li>
</ul>
</li>
</ol>
<p>More prople to add..</p>
]]></content>
      
        <categories>
            
            <category> Re-Id </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Computer Vision </tag>
            
            <tag> Re-Id resources </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[My first Kaggle challenge]]></title>
      <url>/en/2018/05/15/My%201st%20Kaggle%20challenge/</url>
      <content type="html"><![CDATA[<p>This article was written after finishing my first <a href="https://www.kaggle.com" target="_blank" rel="noopener">Kaggle</a> challenge - <a href="https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection" target="_blank" rel="noopener">TalkingData AdTracking Fraud Detection Challenge</a> to share what I've learnt from this challenge. In this article, I'll begin with a breif introduction to Kaggle and to TalkingData AdTracking Fraud Detection Challenge, then I will talk about what I've learnt after this challenge.</p>
<h1>Kaggle Platform</h1>
<p><img src="http://ww1.sinaimg.cn/large/b66c02d5gy1frb9k4qbqpj20pk05k0su.jpg" alt=""></p>
<p>According to Wikipedia,</p>
<blockquote>
<p>Kaggle is a platform for predictive modelling and analytics competitions in which statisticians and data miners compete to produce the best models for predicting and describing the datasets uploaded by companies and users. This crowdsourcing approach relies on the fact that there are countless strategies that can be applied to any predictive modelling task and it is impossible to know beforehand which technique or analyst will be most effective.</p>
</blockquote>
<p>Recently, people added more functionalities to Kaggle, such as sharing your code in &quot;Kernels&quot;, asking a question in &quot;Discussions&quot;, learning a new Data Science technique in &quot;Learn&quot; and finding your job in &quot;Jobs&quot; etc. Kaggle has became the # 1 platform for data scientists and machine learners.</p>
<h1>TalkingData AdTracking Fraud Detection Challenge</h1>
<p><img src="http://ww1.sinaimg.cn/large/b66c02d5gy1frc3q2euw7j20cc036glh.jpg" alt=""></p>
<p>Fraud risk is everywhere, but for companies that advertise online, click fraud can happen at an overwhelming volume, resulting in misleading click data and wasted money. Ad channels can drive up costs by simply clicking on the ad at a large scale. With over 1 billion smart mobile devices in active use every month, China is the largest mobile market in the world and therefore suffers from huge volumes of fradulent traffic.</p>
<p>TalkingData, China’s largest independent big data service platform, covers over 70% of active mobile devices nationwide. They handle 3 billion clicks per day, of which 90% are potentially fraudulent. Their current approach to prevent click fraud for app developers is to measure the journey of a user’s click across their portfolio, and flag IP addresses who produce lots of clicks, but never end up installing apps. With this information, they've built an IP blacklist and device blacklist.</p>
<p>While successful, they want to always be one step ahead of fraudsters and have turned to the Kaggle community for help in further developing their solution. In their 2nd competition with Kaggle, you’re challenged to build an algorithm that predicts whether a user will download an app after clicking a mobile app ad. To support your modeling, they have provided a generous dataset covering approximately 200 million clicks over 4 days!</p>
<h1>Steps of finishing a challenge</h1>
<p>There are 4 crucial steps with their importances:</p>
<ul>
<li>80% feature engineering</li>
<li>10% making local validation as fast as possible</li>
<li>5% hyper parameter tuning</li>
<li>5% ensembling</li>
</ul>
<h2>Feature engineering</h2>
<p>The most important step for winning a challenge is the feature engineering which can take up 80% importances. For a better prediction, you need add up all features that you can find or compose from your original dataset, such as Group by, unique, count, cumulative_count, sort, shift(next &amp; previous), mean, variance, etc.</p>
<h2>Making local validation</h2>
<p>The next step is to make a local validation set as fast as possible. Once it  is created, you can tune your hyperparameters and test your feature importances on your local validation set.</p>
<p>If you don't have a good validation scheme then you rely solely on LB probing, which can easily lead to overfit.</p>
<h2>Hyper parameter tuning</h2>
<p>Two most used methods are XGBoost and Light GBM, you can refer to this website <a href="https://sites.google.com/view/lauraepp/parameters?authuser=0" target="_blank" rel="noopener">xgboost/Light GBM parameters</a> to set up all your parameters quickly.</p>
<p>To get a better accuracy, you need to tune your parameters with <a href="http://scikit-learn.org/stable/modules/grid_search.html" target="_blank" rel="noopener">Gird Search</a>. For example:</p>
<pre><code>param_grid = {'n_estimators': [300, 500], 'max_features': [10, 12, 14]}

model = grid_search.GridSearchCV(estimator=rfr, param_grid=param_grid, n_jobs=1, cv=10, verbose=20, scoring=RMSE)

model.fit(X_train, y_train)
</code></pre>
<h2>Ensemble</h2>
<p>It is more and more difficult to get good predictions with just a single model. If we can combine several medels, the prediction result could usually be better.</p>
<p>There are some ensemble methods:</p>
<ul>
<li><strong>Bagging</strong>: Each Base Model is trained using different random subsets of the training data, and the final results are voted by each base model with the same weight. That is the principle of Random Forest.</li>
<li><strong>Boosting</strong>: The Base Model is trained iteratively, each time modifying the weight of the training sample based on the prediction errors in the previous iteration. This is also the principle of Gradient Boosting. Better than Bagging but easier to Overfit.</li>
<li><strong>Blending</strong>: Different Base Models are trained with disjoint data and their outputs are averaged (weighted). Simple implementation, but less use of training data.</li>
<li><strong>Stacking</strong>: The whole process is very much like Cross Validation. First, the training data is divided into 5 parts, followed by a total of 5 iterations. At each iteration, 4 data are trained as Training Sets for each Base Model, and then the remaining Hold-out Set is used for prediction. At the same time, its predictions on test data should also be preserved. In this way, each Base Model will make a prediction of one of the training data at each iteration and make a prediction of all the test data. After 5 iterations have been completed, we have obtained a matrix of # training data rows x #Base Model number. This matrix is ​​then used as the training data of the second-level Model. After the second-level Model is trained, the previously saved Base Model predicts the test data (because each Base Model has been trained five times, and the entire test data has been predicted five times, so for these five requests An average value, which results in a matrix with the same shape as the second-level training data) is taken out for prediction and the final output is obtained.<img src="https://static.leiphone.com/uploads/new/article/740_740/201703/58cbae21d1aee.jpg?imageMogr2/format/jpg/quality/90" alt=""></li>
</ul>
<h1>Conclusion</h1>
<p>Kaggle is possibly the best platform for data scientists to practice their skills. To start with this challenge, I got inspired from kernels and discussions. Then I tried Light GBM and XGBoost that I had never used before. I'll continue fighting on Kaggle!</p>
<h1>References</h1>
<ol>
<li>
<p><a href="https://www.leiphone.com/news/201703/kCMQyffeP0qUgD9a.html" target="_blank" rel="noopener">TOP5%Kaggler: How to get into top 10% ranking(In Chinese)</a></p>
</li>
<li>
<p><a href="https://www.kaggle.com/c/talkingdata-adtracking-fraud-detection/discussion/56283" target="_blank" rel="noopener">Solution #6 overview</a></p>
</li>
<li>
<p><a href="https://sites.google.com/view/lauraepp/parameters?authuser=0" target="_blank" rel="noopener">xgboost/Light GBM parameters</a></p>
</li>
</ol>
]]></content>
      
        <categories>
            
            <category> Kaggle </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Kaggle </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[A brief introduction to Chatbots with Dialogflow]]></title>
      <url>/en/2018/04/29/A%20brief%20introduction%20to%20Chatbots%20with%20DialogFlow/</url>
      <content type="html"><![CDATA[<p>Nowadays, I'm working on a Chatbot project with Google Dialogflow. In this article, I will present some notions about Dialogflow and Chatbots. Then, I will talk about how to create a simple Chatbot with Dialogflow Platform.</p>
<h1>What is Dialogflow?</h1>
<p><img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fqrbon8ayyj20zk09fdgh.jpg" alt=""></p>
<p>According to Wikipedia:</p>
<blockquote>
<p>Dialogflow (formerly Api.ai, Speaktoit) is a Google-owned developer of human–computer interaction technologies based on natural language conversations. The company is best known for creating the Assistant (by Speaktoit), a virtual buddy for Android, iOS, and Windows Phone smartphones that performs tasks and answers users' question in a natural language. Speaktoit has also created a natural language processing engine that incorporates conversation context like dialogue history, location and user preferences.</p>
</blockquote>
<p>DialogFlow is quite a good choice to learn how to create Chatbots that you can then integrate them into your own websites or Apps.</p>
<h1>Why choose Dialogflow?</h1>
<p>There are several reasons for choosing Dialogflow:</p>
<ol>
<li>
<p><strong>Price</strong></p>
<p>If you just want to learn building a Chatbot or you don't have many users, a Standard Edition is totally free. As you can see below
<img src="http://ww1.sinaimg.cn/large/b66c02d5gy1fqrcnsvpo6j20u20h1ac0.jpg" alt=""></p>
</li>
<li>
<p><strong>Multi-channel easy integration</strong></p>
<p>Dialogflos provides one-click integrations to most popular messaging Apps like Facebook Messenger, Slack, Twitter, Kik, Line, Skype, Telegram, Twilio and Viber. Even to some voice assistants like Google Assistant, Amazon Alexa and Microsoft Cortana.</p>
</li>
<li>
<p><strong>Natural Language Processing(NLP)</strong></p>
<p>Compared to some platforms which works on predefined questions like Chatfuel, Dialogflow can offer better user experience with NLP. DialogFlow Agents are pretty good at NLP.</p>
</li>
</ol>
<h1>How do Chatbots work?</h1>
<p><img src="http://ww1.sinaimg.cn/large/b66c02d5gy1fqrj0wrppej20z50e8419.jpg" alt=""></p>
<p>There are detailed steps:</p>
<ol>
<li>A user sends a text/voice message to a device or an App</li>
<li>The App/Device transfers the message to Dialogflow</li>
<li>The message is categorized and matched to a corresponding intent (Intents are defined manually by developers in Dialogflow)</li>
<li>We define following actions for each intent in the fulfillment (Webhook).</li>
<li>When a certain intent is found by Dialogflow, the webhook will use external APIs to find a response in external data bases.</li>
<li>The external data bases send back required information to the webhook.</li>
<li>Webhook sends formatted response to the intent.</li>
<li>Intent generates actionable data acoording to different channels.</li>
<li>The actionable data go to output Apps/Devices.</li>
<li>The user gets a text/image/voice response.</li>
</ol>
<h1>How to build your first chatbot?</h1>
<h2>Create an agent</h2>
<ol>
<li>If you don't already have a Dialogflow account, sign up. If you have an account, login.</li>
<li>Click on <strong>Create Agent</strong> in the left navigation and fill in the fields.
<img src="http://ww1.sinaimg.cn/large/b66c02d5gy1fqrm0d6cv2j20jr0famxc.jpg" alt=""></li>
<li>Give a name and set a language to your agent. Then, click the Save button.</li>
</ol>
<h2>Create an intent</h2>
<p>An intent maps what a user says with what your agent does. This first intent will cover when the user asks for the weather.</p>
<p>To create an intent:</p>
<ol>
<li>
<p>Click on the plus icon add next to Intents. You will notice some default intents are already in your agent. Just leave them be for now.</p>
</li>
<li>
<p>Enter a name for your intent. This can be whatever you'd like, but it should be intuitive for what the intent is going to accomplish.</p>
</li>
<li>
<p>In the Training Phrases section, enter examples of what you might expect a user to ask for. Since you're creating a weather agent, you want to include questions about locations and different times. The more examples you provide, the more ways a user can ask a question and the agent will understand.</p>
<p>Enter these examples:</p>
<ul>
<li>What is the weather like</li>
<li>What is the weather supposed to be</li>
<li>Weather forecast</li>
<li>What is the weather today</li>
<li>Weather for tomorrow</li>
<li>Weather forecast in San Francisco tomorrow</li>
</ul>
<p>In the last three examples you'll notice the words today and tomorrow are highlighted with one color, and San Francisco is highlighted with another. This means they were annotated as parameters that are assigned to existing date and city system entities. These date and city parameters allow Dialogflow to understand other dates and cities the user may say, and not just &quot;today&quot;, &quot;tomorrow&quot;, and &quot;San Francisco&quot;.
<img src="http://ww1.sinaimg.cn/large/b66c02d5gy1fqrmeevmr5j20h70frt91.jpg" alt=""></p>
</li>
<li>
<p>Click <strong>Save</strong></p>
</li>
</ol>
<h2>Add response</h2>
<p>Now you'll add basic responses to the intent so the agent doesn't just sit there in awkward silence. As mentioned before, responses added to an intent don't use external information. So this will only address the information the agent gathered from the user's request.</p>
<p>If you've navigated away from the &quot;weather&quot; intent, return to it by clicking on Intents and then the &quot;weather&quot; intent.</p>
<ol>
<li>
<p>In the same way you entered the Training Phrases, add the lines of text below in the Response section:</p>
<ul>
<li>Sorry I don't know the weather</li>
<li>I'm not sure about the weather on <code>$date</code></li>
<li>I don't know the weather for <code>$date</code> in <code>$geo-city</code> but I hope it's nice!</li>
</ul>
<p>You can see the last two responses reference entities by their value placeholders. <code>$date</code> will insert the date from the request, and <code>$geo-city</code> will insert the city.</p>
<p>When the agent responds, it takes into account the parameter values gathered and will use a reply that includes those values it picked up. For example, if the request only includes a date, the agent will use the second response from the list.</p>
<p><img src="http://ww1.sinaimg.cn/large/b66c02d5gy1fqrmmcsqj9j20gt08odfy.jpg" alt=""></p>
</li>
<li>
<p>Click <strong>Save</strong></p>
</li>
</ol>
<h2>Try it out</h2>
<p>In the console on the right, type in a request. The request should be a little different than the examples you provided in the Training Phrases section. This can be something like &quot;How's the weather in Denver tomorrow&quot;. After you type the request, hit &quot;Enter/Return&quot;.</p>
<ul>
<li>Response - shows an appropriate response from the ones provided
<ul>
<li>The response chosen is based off of the values you provide in the query (e.g. By providing only the date, the agent should respond with the option that only includes the date)</li>
</ul>
</li>
<li>Intent - weather again a successful trigger of the intent</li>
<li>Parameter - the values you provided in your query, should be reflected in the appropriate response
<img src="http://ww1.sinaimg.cn/large/b66c02d5gy1fqrn3vd2c9j208r0hdmx9.jpg" alt=""></li>
</ul>
<h2>Basic Fulfillment(Webhook)</h2>
<p>Depending on your need, you can define custom Webhook to use external APIs and get extra data from external databases. A webhook can be written in Python and Node.js. It can be held in cloud servers like Heroku, AWS and Google Cloud Platform, and local servers. Please choose your favorite programing language and server:
<a href="">https://dialogflow.com/docs/getting-started/basic-fulfillment-conversation</a></p>
<h2>Integration to your channels</h2>
<p>Dialogflos provides one-click integrations to most popular messaging Apps and voice assistants. Integrations can be done within a few seconds.You can choose your favorite channels and follow the official integration tutorial on the Dialogflow website. Please see:
<a href="">https://dialogflow.com/docs/integrations/</a></p>
<h1>Conclusion</h1>
<p>In this article, you have seen what is Dialogflow, some advantages of choosing this platform, how a Chatbot works within Dialogflow and how to create your first Chatbot. Thanks for your reading. Do not hesitate to contact me if there is any problem.</p>
<h1>References</h1>
<ol>
<li><a href="https://dialogflow.com/docs/getting-started/basics" target="_blank" rel="noopener">Dialogflow Quick Starts</a></li>
<li><a href="https://chatbotslife.com/api-ai-lets-create-a-movie-chatbot-in-minutes-f68d8bb568f9" target="_blank" rel="noopener">DialogFlow(formerly API.AI): Let’s create a Movie ChatBot in minutes
</a></li>
</ol>
]]></content>
      
        <categories>
            
            <category> Chatbot </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Dialogflow </tag>
            
            <tag> Chatbot </tag>
            
            <tag> NLP </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Running Jupyter Notebook on Google Cloud for a Kaggle challenge]]></title>
      <url>/en/2018/04/18/Jupyter%20on%20GCP/</url>
      <content type="html"><![CDATA[<p>When you start a Kaggle challenge, a computer is usually needed to hold all dataset in the memory and accelerate the training with your GPU. Rather than purchasing a new computer,  I'd like to do it free with 300$ credit offered by Google Cloud Platform.</p>
<h1>Step 1: Create a free account in Google Cloud</h1>
<p>For this step, you can create a new Google Account or sign in with your Google Account on https://cloud.google.com/. Then, you will have to put your payment information and verify your account.</p>
<h1>Step 2 : Create a new project on GCP</h1>
<p>Click on the three dots shown in the image below and then click on the + sign to create a new project.
<img src="http://ww1.sinaimg.cn/large/b66c02d5gy1fqgzqonxylj20zn0jymys.jpg" alt=""></p>
<h1>Step 3 : Create a VM instance</h1>
<p>Click on the three lines on the upper left corner, then on the compute option, click on ‘Compute Engine’</p>
<p><img src="http://ww1.sinaimg.cn/large/b66c02d5gy1fqgzurimk3j20ij09vgm2.jpg" alt=""></p>
<p>Now click on ‘Create new instance’. Name your instance, select a zone close to you, in my case, I chose 'europe-west1-b' .Choose your ‘machine type’. (I chose 8v CPUs 52 GB memory because i had a huge dataset). GCP will give you a estimated price according to your configurations.</p>
<p><img src="http://ww1.sinaimg.cn/large/b66c02d5gy1fqh00w2994j20rh09dq3g.jpg" alt=""></p>
<p>You can also customize your vitual machine if you need GPUs. Attention, GPUs are available only in several zones. So , make sure that you have chosen a zone from below:</p>
<ul>
<li>us-west1-b</li>
<li>us-central1-c</li>
<li>us-central1-f</li>
<li>us-east1-c</li>
<li>europe-west1-b</li>
<li>europe-west1-d</li>
<li>asia-east1-a</li>
<li>asia-east1-c</li>
<li>europe-west4-a</li>
</ul>
<p><img src="http://ww1.sinaimg.cn/large/b66c02d5gy1fqh09m85ddj20d70gcq3q.jpg" alt=""></p>
<p>Under Boot Disk option, select your os as ‘Ubuntu 16.04 LTS’ and your disk size as what you need for your datasets, for example, I need 50 GB.</p>
<p><img src="http://ww1.sinaimg.cn/large/b66c02d5gy1fqh0hj7da8j20m70r5goz.jpg" alt=""></p>
<p>Under the firewall options tick both ‘http’ and ‘https’ (very important). Then, choose the disk tab and untick ‘ Delete boot disk when instance is deleted’.</p>
<p><img src="http://ww1.sinaimg.cn/large/b66c02d5gy1fqh0j71xh6j20ei0eyjs4.jpg" alt=""></p>
<p>Now click on ‘Create’ and your instance is ready!</p>
<p><img src="http://ww1.sinaimg.cn/large/b66c02d5gy1fqh35wku3uj20ur0cjt9a.jpg" alt=""></p>
<p><strong>IMPORTANT : DON’T FORGET TO STOP YOUR GPU INSTANCE AFTER YOU ARE DONE BY CLICKING ON THE THREE DOTS ON THE IMAGE ABOVE AND SELECTING STOP. OTHERWISE GCP WILL KEEP CHARGING YOU ON AN HOURLY BASIS.</strong></p>
<h1>Step 4: Make external IP address as static</h1>
<p>By default, the external IP address is dynamic and we need to make it static to make our life easier. Click on the three horizontal lines on top left and then under networking, click on VPC network and then External IP addresses.</p>
<p><img src="http://ww1.sinaimg.cn/large/b66c02d5gy1fqh3esd116j20cn0n13zp.jpg" alt=""></p>
<p>Change the type from Ephemeral to Static.</p>
<p><img src="http://ww1.sinaimg.cn/large/b66c02d5gy1fqh3i1zgulj20og03ydfu.jpg" alt=""></p>
<p>Now, click on the ‘Firewall rules’ setting under VPC network. Click on ‘Create Firewall Rules’ and refer the below image:</p>
<p><img src="http://ww1.sinaimg.cn/large/b66c02d5gy1fqh3pd7lmjj20gi0owmxz.jpg" alt=""></p>
<p>Under protocols and ports you can choose any port. I have chosen tcp:5000 as my port number. Now click on the save button.</p>
<h1>Step 5 : Install Google Cloud SDK</h1>
<p>According to your OS, refer the corresponding document on https://cloud.google.com/sdk/docs/quickstarts</p>
<p>Then run
<code>gcloud init</code>
follow steps on yhe website to initialize your Google Cloud SDK.</p>
<h1>Step 6 : Install Jupyter notebook and other packages</h1>
<p>Open a terminal, connect to your VM instance:</p>
<p><code>gcloud compute --project &lt;project name&gt; ssh --zone &lt;zone name&gt; &lt;instance name&gt;</code></p>
<p>Then, install anaconda3 into your VM,</p>
<pre><code>wget http://repo.continuum.io/archive/Anaconda3-5.1.0-Linux-x86_64.sh	

bash Anaconda3-5.1.0-Linux-x86_64.sh
</code></pre>
<p>and follow the on-screen instructions. The defaults usually work fine, but answer yes to the last question about prepending the install location to PATH:</p>
<pre><code>Do you wish the installer to prepend the 
Anaconda3 install location to PATH 
in your /home/haroldsoh/.bashrc ? 
[yes|no][no] &gt;&gt;&gt; yes
</code></pre>
<p>To make use of Anaconda right away, source your bashrc:</p>
<pre><code>source ~/.bashrc
</code></pre>
<p>Now, install other softwares, for example</p>
<pre><code>conda install -c conda-forge lightgbm 
</code></pre>
<h1>Step 7: Set up the VM server</h1>
<p>Open up a SSH session to your VM. Check if you have a Jupyter configuration file:</p>
<pre><code>ls ~/.jupyter/jupyter_notebook_config.py
</code></pre>
<p>If it doesn’t exist, create one:</p>
<pre><code>jupyter notebook --generate-config
</code></pre>
<p>We’re going to add a few lines to your Jupyter configuration file; the file is plain text so, you can do this via your favorite editor (e.g., vim, emacs). Make sure you replace the port number with the one you allowed firewall access to in step 4.</p>
<p>On Mac, a folder name that starts by a dot is a hidden floder. In a text editor, to view hidden files and folders in the Open/Save dialog, just press Command+Shift+Period (that’s the . key).</p>
<pre><code>c = get_config()
c.NotebookApp.ip = '*'
c.NotebookApp.open_browser = False
c.NotebookApp.port = &lt;Port Number&gt;
</code></pre>
<p>It should look something like this :</p>
<p><img src="http://ww1.sinaimg.cn/large/b66c02d5gy1fqh4q4uy1yj20s40jgmyq.jpg" alt=""></p>
<h1>Step 8 : Launching Jupyter Notebook</h1>
<p>To run the jupyter notebook, just type the following command in the ssh window you are in :</p>
<pre><code>jupyter notebook --ip=0.0.0.0 --port=&lt;port-number&gt; --no-browser &amp;
</code></pre>
<p>Once you run the command, it gives you a token like this:</p>
<p><img src="http://ww1.sinaimg.cn/large/b66c02d5gy1fqh4x9syeaj20k20a3ac6.jpg" alt=""></p>
<p>Now to launch your jupyter notebook, just type the following in your browser :</p>
<pre><code>http://&lt;External Static IP Address&gt;:&lt;Port Number&gt;
</code></pre>
<p>where, external ip address is the ip address which we made static and port number is the one which we allowed firewall access to.</p>
<p>Enter the token you got in the last step:</p>
<p><img src="http://ww1.sinaimg.cn/large/b66c02d5gy1fqh57tvcx7j20rl0mxgn4.jpg" alt=""></p>
<p>Then you have a jupyter notebook running on GCP.</p>
<p><img src="http://ww1.sinaimg.cn/large/b66c02d5gy1fqh5amoxfgj21az0f2q3f.jpg" alt=""></p>
<h1>References</h1>
<ol>
<li>
<p><a href="https://towardsdatascience.com/running-jupyter-notebook-in-google-cloud-platform-in-15-min-61e16da34d52" target="_blank" rel="noopener">Running Jupyter Notebook on Google Cloud Platform in 15 min</a></p>
</li>
<li>
<p><a href="https://cloud.google.com/sdk/docs/quickstarts" target="_blank" rel="noopener">Google Cloud Quickstarts</a></p>
</li>
</ol>
]]></content>
      
        <categories>
            
            <category> Kaggle </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Kaggle </tag>
            
            <tag> Google Cloud Platform </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Sequence Model week3]]></title>
      <url>/en/2018/03/15/Sequence-Model-week3/</url>
      <content type="html"><![CDATA[<h1>Various sequence to sequence qrchitectures</h1>
<h2>Basic Models</h2>
<h3>Machine translation</h3>
<p>For a machine tanslation, you need a encoder-decoder model. In this model, each block can represent a RNN/LSTM/GRU.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fpbj7k98blj20qo0f0gq5.jpg" alt=""></p>
<h3>Image captioning</h3>
<p>How to interpret an image into a caption? In the example, we use a pre-trainde Alex network as encoders. Then we also need sevaral RNN blocks as decoders.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fpbjl2nlhpj20qo0f0dmd.jpg" alt=""></p>
<h2>Picking the most likely sentence</h2>
<p>A machine translation problem can be regarded as a conditional language model, and &quot;conditional&quot; means given the probability of the original sentence.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fpbkgx0oo6j20qo0f0gr4.jpg" alt=""></p>
<p>###Why not a greedy search?
<strong>greedy search:</strong> find the most likely word for the position 1, then for the position 2, etc..
It doesn't work well when we need to translate a whole sentence.</p>
<h2>Beam Search</h2>
<h3>step 1</h3>
<p>B=3 means that 3 most likely words are chosen.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fpblyt59uwj20qo0f0ad0.jpg" alt=""></p>
<h3>step 2</h3>
<p>For each candidate word chosen in step 1, run RNN blocks to find out the most likely pairs of $y^{&lt;1&gt;},y^{&lt;2&gt;}$, then cut down these 30000 possibilities into 3 most possible.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fpbmkcxpj0j20qo0f0gv4.jpg" alt=""></p>
<h3>step 3</h3>
<p>Same pocesse like step 2.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fpbml4fkw6j20qo0f0jw6.jpg" alt=""></p>
<h2>Refinements to Beam Search</h2>
<h3>Length normalization</h3>
<p>$P(y^{&lt;1&gt;}...y^{&lt;T_y&gt;}|x)$ can be thought as a product of many conditional probabilities, which can be too small.</p>
<p>The result of model tends to be short sentences, because the product of probability of short sentence is likely to be bigger.</p>
<p>To avoid this disadvantage, one approach is dividing this product by number of words in the sentence.
<img src="http://ww1.sinaimg.cn/large/b66c02d5gy1fpcwk14z1lj20qo0f0wjq.jpg" alt=""></p>
<h3>Beam search discussion</h3>
<p>Beam width B is a compromise of performance and speed.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fpcwmzdh8tj20qo0f0gpn.jpg" alt=""></p>
<h2>Error analysis in beam search</h2>
<p>When there is an error, we want to be clear this error comes from RNN or beam search.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fpdsgsmf59j20qo0f0tez.jpg" alt=""></p>
<h2>Blue Score</h2>
<p>There may be several possible translation for a sentence.  How to choose one? A way is to count the max number of words that appear in the reference.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fpdvn5qrkwj20qo0f0teo.jpg" alt=""></p>
<p>Blue score on bigrams
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fpdwaxawzsj20qo0f0q7m.jpg" alt=""></p>
<p>Blue score on unigrams
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fpdwcv6isoj20qo0f0wki.jpg" alt=""></p>
<p>BP= brevity penalty
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fpdwlzjnqbj20qo0f0tcu.jpg" alt=""></p>
<p>##Attention model Intuition
Encoder-decoder model can lose performance when a sentence is too long.</p>
<p>In a bidirectional RNN, you can add an attention weight to each word, which measures how much attention you should pay attention to this word. Then these weights are sent to another RNN to evaluate attentions.</p>
<p><img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fpdxp784xwj20qo0f0gst.jpg" alt=""></p>
<p>More clearly, $\alpha^{&lt;t,t\prime&gt;}= amount\ of\ attention\ that\ y^{&lt;t&gt;}\ should\ pay\ to\ a^{&lt;t\prime&gt;}$</p>
<p><img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fpe09cy2tvj20qo0f0n4a.jpg" alt=""></p>
<h3>Computing attention</h3>
<p>Use a small NN to get proper e
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fpe0gosl03j20qo0f0tet.jpg" alt=""></p>
<h1>Speech recognition - Audio Data</h1>
<h2>Speech recognition</h2>
<p>We can also apply an attention model to speech recognition.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fpetgs5x0kj20qo0f00v5.jpg" alt=""></p>
<p>Another approach is Connectionist temporal classification.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fpetoydz9bj20qo0f0jwm.jpg" alt=""></p>
<h2>Trigger Word Detection</h2>
<p>Use 0/1 to distinguish trigger words or nornam words.</p>
<p><img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fpeu5v60t2j20qo0f0dk9.jpg" alt=""></p>
]]></content>
      
        <categories>
            
            <category> Coursera </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Sequence Model week2]]></title>
      <url>/en/2018/03/10/Sequence-Model-week2/</url>
      <content type="html"><![CDATA[<h1>Introduction to Word Embeddings</h1>
<h2>Word Representation</h2>
<h3>One-hot representation</h3>
<p>So far, we have used a vocabulary and one-hot representation
$$Man(5391)= \begin{bmatrix} 0\\ \vdots \\ 0 \\ 1\\0\\ \vdots\\ 0 \end{bmatrix}=O_{5391}$$
$$Woman(9853)= \begin{bmatrix} 0\\ \vdots \\ 0 \\ 1\\0\\ \vdots\\ 0 \end{bmatrix}=O_{9853}$$
It's hard to recognize the relationship between 2 vectors, because inner product of any 2 one-hot vectors is 0.</p>
<h3>Featurized representation: word embedding</h3>
<p>Instead of using one-hot encoding, to represent a word, you can choose some features like Gender, Royal, Age, Food .. Size, Cost.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fp3d8i1h66j20qo0f0n3b.jpg" alt=""></p>
<p>##Using word embeddings
Using transfer learning and word embeddings can be a good approach to slove NLP problems.</p>
<p>Here are the 3 steps that you need to do when considering this approach.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fp3eyj4vo1j20qo0f0q7y.jpg" alt=""></p>
<h2>Properties of word embeddings</h2>
<p>One interesting property is if we take two vectors like $e_{man}$ and $e_{woman}$, $e_{man}-e_{woman}$ can give the main difference between 2 vectors.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fp4h09qz12j20qo0f0dm3.jpg" alt="">
To put this into an algorithm, for example, if you want to find what is the corresponding word of &quot;king&quot; in the way like &quot;man&quot; and &quot;woman&quot;, we need to calculate the similarity.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fp4h5mkc70j20qo0f0n1d.jpg" alt="">
Two similarity calculation methods:</p>
<ol>
<li>** Cosine similarity **</li>
<li>** Euclidian distance **
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fp4hp1pirjj20qo0f0adr.jpg" alt=""></li>
</ol>
<h2>Embedding Matrix</h2>
<p>You can put all Use an embeddinng matrix and one-hot vector can get an embedding vector easily.</p>
<p>But in practice, use special function to look up an embedding.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fp4hytdamvj20qo0f0wkm.jpg" alt=""></p>
<h1>Learning Word Embedddings: Word2vec &amp; GloVe</h1>
<h2>Learning word embedddings</h2>
<p>A learning model can be like: word embedding =&gt; neural network with parameters $w^{[1]}$ and $b^{[1]}$ =&gt; softmax with parameters $w^{[2]}$ and $b^{[2]}$</p>
<p>A common algorithm for learning the matrix E looks like the following image, we want to predict the target word &quot;juice&quot; using context words&quot;a glass of orange&quot;.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fp4l154hc6j20qo0f0grx.jpg" alt="">
Context can vary from last few words to next few words or both. Pairs of context/target are import</p>
<h2>Word2Vec Skip-gram model</h2>
<p>Word2Vec is a simple way to learn word embeddings.</p>
<p>When apply Skip-grams for context/target selection, we randomly pick up words in a window as context/target.</p>
<p><img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fp4osyx745j20qo0f078i.jpg" alt=""></p>
<p>A problem with softmax classification, every time you need a propobility of a target word, you calculate a sum whose length equals to that of vocabulary. When you have many words in the vocabulary, it can be very slow.</p>
<p>So instead of using softmax classification, hierarchical softmax can be a more proper way. You can use a tree to figure out that a word locates in which part of a vocabulary.</p>
<p>For instance, the target can be chosen within a windows of -10/10 words. But choosing a context can be more tricky, you should avoid using frequent non-sense words like &quot;the&quot; &quot;and&quot; &quot;to&quot; &quot;a&quot;.</p>
<p>A even simpler solution to this problem is Negative Sampling.</p>
<h2>Negative Sampling</h2>
<p>We create a supervised problem, given pairs of context/target as x, y should matching results in 0/1. To form a dataset, 5-20 pairs for small datasets and 2-5 pairs for larger datasets.</p>
<p><img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fp4qxi3hxvj20qo0f0dkw.jpg" alt=""></p>
<p>We only train the model with 1 correct pair and 4 other incorrect pairs.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fp4r8vaxjsj20qo0f0jw0.jpg" alt=""></p>
<p>To select negative samples, the authur proposed a way:
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fp4rb9bz9qj20qo0f0goi.jpg" alt=""></p>
<h2>GloVe word vectors</h2>
<p>We define pairs of context/target in the following way:
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fp5k889htuj20qo0f077t.jpg" alt=""></p>
<p>The model looks like:
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fp5ki84tauj20qo0f0q68.jpg" alt="">
You can't guarantee that an axis is well aligned to a feature.</p>
<p>#Allications using Word Embeddings</p>
<h2>Sentiment Classification</h2>
<p>A simple sentiment classification model takes the average of each word embedding vector.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fp5liyzosxj20qo0f0q7i.jpg" alt=""></p>
<p>Use a RNN can avoid disadvantages of some special sentences.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fp5llahpgvj20qo0f0whs.jpg" alt=""></p>
<h2>Debiasing word embeddings</h2>
<p>What is bias in word embedding?
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fp5pnb2omxj20qo0f0wjg.jpg" alt="">
<img src="http://ww1.sinaimg.cn/large/b66c02d5gy1fp5rtzo750j20qo0f00yd.jpg" alt=""></p>
]]></content>
      
        <categories>
            
            <category> Coursera </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Sequence Model week1]]></title>
      <url>/en/2018/03/05/Sequence-Model-week1/</url>
      <content type="html"><![CDATA[<h1>Why sequence models</h1>
<h2>Examples of sequence data</h2>
<ol>
<li>speech recognition</li>
<li>Music generation</li>
<li>Sentiment classification</li>
<li>DNA sequence analysis</li>
<li>Machine translation</li>
<li>Video activity recognition</li>
<li>Name entity recognition</li>
</ol>
<h1>Notation</h1>
<h2>Motivating example</h2>
<p>Name entity recognition</p>
<table>
<thead>
<tr>
<th>x</th>
<th>Harry</th>
<th>Potter</th>
<th>and</th>
<th>Hermione</th>
<th>Granger</th>
<th>invented</th>
<th>a</th>
<th>new</th>
<th>spell.</th>
</tr>
</thead>
<tbody>
<tr>
<td>y</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>$$x^{&lt;1&gt;} \to x^{&lt;9&gt;} stand\ for\ the\ 1st\ to\ 9th\ element\ in\ x\ vector, T_x=9\ length=9$$</p>
<p>$$y^{&lt;1&gt;} \to y^{&lt;9&gt;} stand\ for\ the\ 1st\ to\ 9th\ element\ in\ y\ vector, T_y=9\ length =9$$</p>
<p>$x^{(i)&lt;t&gt;}$ refers to $(i)$th training example's $&lt;t&gt;$th element.</p>
<p>$T_x^{(i)}$ would be the input sequence length for training example i.</p>
<h2>Representing words</h2>
<p>To represent words in a sentence, firstly we need a Vocabulary/Dictionary which is a list of the words that you will use in your representations.</p>
<table>
<thead>
<tr>
<th>a</th>
<th>aaron</th>
<th>...</th>
<th>and</th>
<th>...</th>
<th>harry</th>
<th>...</th>
<th>potter</th>
<th>...</th>
<th>zulu</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>2</td>
<td>...</td>
<td>367</td>
<td>...</td>
<td>4075</td>
<td>...</td>
<td>6830</td>
<td>...</td>
<td>10000</td>
</tr>
</tbody>
</table>
<p>Use one-hot encoding</p>
<p>$$x^{&lt;1&gt;}= \begin{bmatrix}  0\\   \vdots \\   0\\ 1\leftarrow4075\\ 0\\   \vdots  \\  0   \end{bmatrix};
x^{&lt;2&gt;}=\begin{bmatrix}  0\\   \vdots \\   0\\ 1\leftarrow6830\\ 0\\   \vdots  \\  0   \end{bmatrix};
x^{&lt;3&gt;}=\begin{bmatrix}  0\\   \vdots \\   0\\ 1\leftarrow367\\ 0\\   \vdots  \\  0   \end{bmatrix};
x^{&lt;7&gt;}=\begin{bmatrix}  1\\   \vdots \\   0\\ 0\\ 0\\   \vdots  \\  0   \end{bmatrix}$$
When a word can't be found in the Vocabulary, we shall add $&lt;UNK&gt;$ to represent this word.</p>
<h1>Recurrent Neural Networks</h1>
<h2>Why not a standard network?</h2>
<p>Some problems:</p>
<ul>
<li>Inputs, outputs can be different lengths in different examples.</li>
<li>Doesn't share features learned across different positions of text.</li>
</ul>
<h2>Recurrent Neural Networks</h2>
<p>$y^{&lt;i&gt;}$ not only depends on the current input$x^{&lt;i&gt;}$, but also depends on former inputs $x^{&lt;i-1&gt;} \dots x^{1}$.</p>
<p>Each layer passes an activation function $a^{&lt;i&gt;}$ to next layer $&lt;i+1&gt;$, at the beginning, we will need a $a^{&lt;0&gt;}$ which is usually a vector of zeros.</p>
<h3>Uni-direction vs Bi-direction</h3>
<p>There is a problem for this Uni-diretional RNN:</p>
<ul>
<li>The current level output can only consider current and former inputs.</li>
</ul>
<p>But sometimes you have to look at following words, for example:</p>
<ul>
<li>He said, &quot;Teddy Roosevelt was a great President.&quot;</li>
<li>He said, &quot;Teddy bears are on sale!&quot;</li>
</ul>
<p>In this case, the first three words are same, in order to figure out what &quot;Teddy&quot; exactely refers to, you should take the following words like &quot;Roosevelt&quot; or &quot;bears&quot; into considerations. So, there comes a RNN model that considers both former and following words, which is called Bidirectional RNN(BRNN).</p>
<h2>Forward Propagation</h2>
<p>At the ever beginning
$$a^{&lt;0&gt;}=\vec{0}$$
For the first level
$$a^{&lt;1&gt;}=g_1(w_{aa}a^{&lt;0&gt;}+w_{ax}x^{&lt;1&gt;}+b_a)\leftarrow \underline{tanh}\ or\  ReLU\
\hat y^{&lt;1&gt;}=g_2(w_{ya}a^{&lt;1&gt;}+b_y)\leftarrow\ sigmoid$$
More generally
$$a^{&lt;t&gt;}=g_1(w_{aa}a^{&lt;t-1&gt;}+w_{ax}x^{&lt;t&gt;}+b_a)\leftarrow \underline{tanh}\ or\  ReLU\
\hat y^{&lt;t&gt;}=g_2(w_{ya}a^{&lt;t&gt;}+b_y)\leftarrow\ sigmoid$$</p>
<h3>Simplified RNN notation</h3>
<p>You can write the above formula like:
$$a^{&lt;t&gt;}=g_1(w_{a}\binom{a^{&lt;t-1&gt;}}{x{&lt;t&gt;}}+b_a) \ [w_{aa}\ w_{ax}]=w_a$$</p>
<h2>Backpropagation through time</h2>
<p>Usually, a DL framework can automatically take care of backpropagation.</p>
<p>Defining a loss function to finish the backpropogation.</p>
<h1>Different types of RNNs</h1>
<p>When we have length of input equals to length of output, it's called: <strong>Many-to-many architecture</strong>: $T_x=T_y$.</p>
<p>By contrast, for a sentiment classification problem,$x=text\ ;\ y=0/1 \ or 1..5 $, rather than using every input to have an output, you can just let RNN read the whole sentence and get an output at the last step. This model is called <strong>Many-to-one architecture</strong>.</p>
<p>There is also <strong>one-to-one architecture</strong>, which is maybe less interesting.</p>
<p>In the end, you can have <strong>one-to-many architecture</strong> that can be used for music generation. You provide an integer as the gender of music or as a first key, you get a chapiter of music.</p>
<p>Another example is Machine translation, it's another version of many-to-many architecture, but there are two distinguishable parts: &quot;encoder&quot;(pure inputs) and &quot;decoder&quot;(pure outputs).</p>
<h1>Languages model and sequence generation</h1>
<h2>Speech recognition</h2>
<p>For example, when I say</p>
<blockquote>
<p>The apple and pear salad were delicious.</p>
</blockquote>
<p>even for those best speech recognition systems, it's hard to figure out</p>
<ol>
<li>The apple and pair salad</li>
<li>The apple and pear salad</li>
</ol>
<p>A speech recognition system could give a probability to each sentence. If the second one has a higher probability, it will be chosen.</p>
<h2>language modelling with an RNN</h2>
<p><strong>Trainng set</strong>: large corpus of english text.</p>
<p>You need firstly <strong>tokenize</strong> the text.</p>
<p>Cats average 15 hours of sleep a day. EOS =&gt; $y^{&lt;1&gt;}\ y^{&lt;2&gt;}\ y^{&lt;3&gt;}\ ... y^{&lt;9&gt;}$$$&lt;EOS&gt;= end\ of\ sentence$$</p>
<p>The Egyptian Mau is a bread of cat. The special word Mau=&gt;$&lt;UNK&gt;=Unknown$</p>
<h2>RNN model</h2>
<p>At each step of a RNN network, the output should be a probability of all words in a dictionary given last words.</p>
<p>To train this network, the loss function at step t could be $L(\hat{y}^{&lt;t&gt;},y^{&lt;t&gt;})=-\sum{y_i^{&lt;t&gt;}log\hat{y}_i^{&lt;t&gt;}}$. The overall loss is $L=\sum{L^{&lt;t&gt;}(\hat{y}^{&lt;t&gt;},y^{&lt;t&gt;})}$.</p>
<p>With thismodel, we can predict the next word given a set of words.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fowe70ba7uj20qo0f07b4.jpg" alt=""></p>
<h1>Sampling novel sequences</h1>
<h2>Sampling a sequence from a trained RNN</h2>
<h3>Word-level language model</h3>
<p>Use words as inputs to train RNN models</p>
<h3>Character-level language model</h3>
<p>Use characters as inputs to train RNN models
pros:</p>
<ul>
<li>Don't have to worry about UNK
cons:</li>
<li>you end up with much longer sequences, 10-20 words equal 100-200 characters, so it's harder to capture dependencies from former characters</li>
<li>more computationally expensive to train</li>
</ul>
<h1>Vanishing gradients with RNNs</h1>
<p>Just like deep neural networks, it can be quite difficult to let the later time steps affect the earlier computations. But for RNNs, it's possible to have both vanishing and exploding gradients.</p>
<p>A possible solution for exploding gradients is to apply gradient clipping, which means looking at your gradient vector, if it's bigger than some thresholds, resize these gradient vectors.</p>
<h1>Gated Recurrent Unit(GRU)</h1>
<p>GRU is a possible solution to vanishing gradients.</p>
<h2>GRU(simplified)</h2>
<p>For example, there is a sentence &quot;The cat, which already ate... , was full &quot;, you have to memorize it's &quot;cat&quot; or &quot;cats&quot; to decide &quot;was&quot; or &quot;were&quot;.
$$c=memory\ cell$$
$$c^{&lt;t&gt;}=a^{&lt;t&gt;}$$
Then there is a candidate for replacing the memory cell:
$$\tilde{c}^{&lt;t&gt;}=tanh(w_c[c^{&lt;t-1&gt;},x^{&lt;t&gt;}]+b_c)$$
$$\Gamma_u=\sigma(w_u[c^{&lt;t-1&gt;},x^{&lt;t&gt;}]+b_u), u=update$$
The $\Gamma_u$ should be 1 at the word &quot;cat&quot;, 0 at other words.</p>
<p>For a sigmoid function, the result is a number between 0 and 1, and most time it's either 0 or 1. So it's called <strong>Gate</strong>.</p>
<p>Then use the following function to update memory cells:
$$c^{&lt;t&gt;}=\Gamma_u*\tilde{c}^{&lt;t&gt;}+(1-\Gamma_u)*c^{&lt;t-1&gt;}$$</p>
<p><img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1foxli3a7huj20qo0f0ajs.jpg" alt=""></p>
<h2>Full GRU</h2>
<p>In the full version GRU, we add a new gate $\Gamma_r$ in the candidate function, r stands for relevance. The main structure keeps unchanged.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1foxmfgoxgvj20qo0f0tb5.jpg" alt=""></p>
<h1>Long Short term Memory(LSTM)</h1>
<p>Another solution, maybe even more powerful than GRU in some cases, is LSTM. Instead of using two gates, an update, a forget gate and an output gate are used in LSTM.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1foxte03cmaj20qo0f0ah7.jpg" alt="">
At each time step, we use those three gates to update memory cells.</p>
<p>LSTM came out earlier than GRU, but in different cases, they could have different performances.</p>
<h1>Bidirectional RNN</h1>
<p>In the previous models, there are only forwards connections, but in BRNN, there will be also backwards connections.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1foxty66rg6j20qo0f0agb.jpg" alt="">
A Bidirectional structure with LSTM blocks is a first thing to try for recent NLP problems.</p>
<h1>Deep RNNs</h1>
<p>In a deep RNN, we shall use a chained blocks as a layer. So to compute a block, both horizontal and vertical inputs are needed. Like $a^{[2]&lt;3&gt;}=g(w_a^{[2]}[a^{[2]&lt;2&gt;},a^{[1]&lt;3&gt;}]+b_a^{[3]})$
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1foxu8xx22fj20qo0f0grx.jpg" alt=""></p>
<p>Usually there aren't many horizontal layers, but we add some vertical connections at the last layer outputs.</p>
]]></content>
      
        <categories>
            
            <category> Coursera </category>
            
        </categories>
        
        
        <tags>
            
            <tag> NLP </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
        
    </entry>
    
  
  
</search>
