<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title><![CDATA[Sequence Model week2]]></title>
      <url>/en/2018/03/10/Sequence-Model-week2/</url>
      <content type="html"><![CDATA[<h1>Introduction to Word Embeddings</h1>
<h2>Word Representation</h2>
<h3>One-hot representation</h3>
<p>So far, we have used a vocabulary and one-hot representation
$$Man(5391)= \begin{bmatrix} 0\\ \vdots \\ 0 \\ 1\\0\\ \vdots\\ 0 \end{bmatrix}=O_{5391}$$
$$Woman(9853)= \begin{bmatrix} 0\\ \vdots \\ 0 \\ 1\\0\\ \vdots\\ 0 \end{bmatrix}=O_{9853}$$
It's hard to recognize the relationship between 2 vectors, because inner product of any 2 one-hot vectors is 0.</p>
<h3>Featurized representation: word embedding</h3>
<p>Instead of using one-hot encoding, to represent a word, you can choose some features like Gender, Royal, Age, Food .. Size, Cost.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fp3d8i1h66j20qo0f0n3b.jpg" alt=""></p>
<p>##Using word embeddings
Using transfer learning and word embeddings can be a good approach to slove NLP problems.</p>
<p>Here are the 3 steps that you need to do when considering this approach.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fp3eyj4vo1j20qo0f0q7y.jpg" alt=""></p>
<h2>Properties of word embeddings</h2>
<p>One interesting property is if we take two vectors like $e_{man}$ and $e_{woman}$, $e_{man}-e_{woman}$ can give the main difference between 2 vectors.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fp4h09qz12j20qo0f0dm3.jpg" alt="">
To put this into an algorithm, for example, if you want to find what is the corresponding word of &quot;king&quot; in the way like &quot;man&quot; and &quot;woman&quot;, we need to calculate the similarity.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fp4h5mkc70j20qo0f0n1d.jpg" alt="">
Two similarity calculation methods:</p>
<ol>
<li>** Cosine similarity **</li>
<li>** Euclidian distance **
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fp4hp1pirjj20qo0f0adr.jpg" alt=""></li>
</ol>
<h2>Embedding Matrix</h2>
<p>You can put all Use an embeddinng matrix and one-hot vector can get an embedding vector easily.</p>
<p>But in practice, use special function to look up an embedding.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fp4hytdamvj20qo0f0wkm.jpg" alt=""></p>
<h1>Learning Word Embedddings: Word2vec &amp; GloVe</h1>
<h2>Learning word embedddings</h2>
<p>A learning model can be like: word embedding =&gt; neural network with parameters $w^{[1]}$ and $b^{[1]}$ =&gt; softmax with parameters $w^{[2]}$ and $b^{[2]}$</p>
<p>A common algorithm for learning the matrix E looks like the following image, we want to predict the target word &quot;juice&quot; using context words&quot;a glass of orange&quot;.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fp4l154hc6j20qo0f0grx.jpg" alt="">
Context can vary from last few words to next few words or both. Pairs of context/target are import</p>
<h2>Word2Vec Skip-gram model</h2>
<p>Word2Vec is a simple way to learn word embeddings.</p>
<p>When apply Skip-grams for context/target selection, we randomly pick up words in a window as context/target.</p>
<p><img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fp4osyx745j20qo0f078i.jpg" alt=""></p>
<p>A problem with softmax classification, every time you need a propobility of a target word, you calculate a sum whose length equals to that of vocabulary. When you have many words in the vocabulary, it can be very slow.</p>
<p>So instead of using softmax classification, hierarchical softmax can be a more proper way. You can use a tree to figure out that a word locates in which part of a vocabulary.</p>
<p>For instance, the target can be chosen within a windows of -10/10 words. But choosing a context can be more tricky, you should avoid using frequent non-sense words like &quot;the&quot; &quot;and&quot; &quot;to&quot; &quot;a&quot;.</p>
<p>A even simpler solution to this problem is Negative Sampling.</p>
<h2>Negative Sampling</h2>
<p>We create a supervised problem, given pairs of context/target as x, y should matching results in 0/1. To form a dataset, 5-20 pairs for small datasets and 2-5 pairs for larger datasets.</p>
<p><img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fp4qxi3hxvj20qo0f0dkw.jpg" alt=""></p>
<p>We only train the model with 1 correct pair and 4 other incorrect pairs.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fp4r8vaxjsj20qo0f0jw0.jpg" alt=""></p>
<p>To select negative samples, the authur proposed a way:
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fp4rb9bz9qj20qo0f0goi.jpg" alt=""></p>
<h2>GloVe word vectors</h2>
<p>We define pairs of context/target in the following way:
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fp5k889htuj20qo0f077t.jpg" alt=""></p>
<p>The model looks like:
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fp5ki84tauj20qo0f0q68.jpg" alt="">
You can't guarantee that an axis is well aligned to a feature.</p>
<p>#Allications using Word Embeddings</p>
<h2>Sentiment Classification</h2>
<p>A simple sentiment classification model takes the average of each word embedding vector.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fp5liyzosxj20qo0f0q7i.jpg" alt=""></p>
<p>Use a RNN can avoid disadvantages of some special sentences.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fp5llahpgvj20qo0f0whs.jpg" alt=""></p>
<h2>Debiasing word embeddings</h2>
<p>What is bias in word embedding?
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fp5pnb2omxj20qo0f0wjg.jpg" alt="">
<img src="http://ww1.sinaimg.cn/large/b66c02d5gy1fp5rtzo750j20qo0f00yd.jpg" alt=""></p>
]]></content>
      
        <categories>
            
            <category> Coursera </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> NLP </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Sequence Model week1]]></title>
      <url>/en/2018/03/05/Sequence-Model-week1/</url>
      <content type="html"><![CDATA[<h1>Why sequence models</h1>
<h2>Examples of sequence data</h2>
<ol>
<li>speech recognition</li>
<li>Music generation</li>
<li>Sentiment classification</li>
<li>DNA sequence analysis</li>
<li>Machine translation</li>
<li>Video activity recognition</li>
<li>Name entity recognition</li>
</ol>
<h1>Notation</h1>
<h2>Motivating example</h2>
<p>Name entity recognition</p>
<table>
<thead>
<tr>
<th>x</th>
<th>Harry</th>
<th>Potter</th>
<th>and</th>
<th>Hermione</th>
<th>Granger</th>
<th>invented</th>
<th>a</th>
<th>new</th>
<th>spell.</th>
</tr>
</thead>
<tbody>
<tr>
<td>y</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>$$x^{&lt;1&gt;} \to x^{&lt;9&gt;} stand\ for\ the\ 1st\ to\ 9th\ element\ in\ x\ vector, T_x=9\ length=9$$</p>
<p>$$y^{&lt;1&gt;} \to y^{&lt;9&gt;} stand\ for\ the\ 1st\ to\ 9th\ element\ in\ y\ vector, T_y=9\ length =9$$</p>
<p>$x^{(i)&lt;t&gt;}$ refers to $(i)$th training example's $&lt;t&gt;$th element.</p>
<p>$T_x^{(i)}$ would be the input sequence length for training example i.</p>
<h2>Representing words</h2>
<p>To represent words in a sentence, firstly we need a Vocabulary/Dictionary which is a list of the words that you will use in your representations.</p>
<table>
<thead>
<tr>
<th>a</th>
<th>aaron</th>
<th>...</th>
<th>and</th>
<th>...</th>
<th>harry</th>
<th>...</th>
<th>potter</th>
<th>...</th>
<th>zulu</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>2</td>
<td>...</td>
<td>367</td>
<td>...</td>
<td>4075</td>
<td>...</td>
<td>6830</td>
<td>...</td>
<td>10000</td>
</tr>
</tbody>
</table>
<p>Use one-hot encoding</p>
<p>$$x^{&lt;1&gt;}= \begin{bmatrix}  0\\   \vdots \\   0\\ 1\leftarrow4075\\ 0\\   \vdots  \\  0   \end{bmatrix};
x^{&lt;2&gt;}=\begin{bmatrix}  0\\   \vdots \\   0\\ 1\leftarrow6830\\ 0\\   \vdots  \\  0   \end{bmatrix};
x^{&lt;3&gt;}=\begin{bmatrix}  0\\   \vdots \\   0\\ 1\leftarrow367\\ 0\\   \vdots  \\  0   \end{bmatrix};
x^{&lt;7&gt;}=\begin{bmatrix}  1\\   \vdots \\   0\\ 0\\ 0\\   \vdots  \\  0   \end{bmatrix}$$
When a word can't be found in the Vocabulary, we shall add $&lt;UNK&gt;$ to represent this word.</p>
<h1>Recurrent Neural Networks</h1>
<h2>Why not a standard network?</h2>
<p>Some problems:</p>
<ul>
<li>Inputs, outputs can be different lengths in different examples.</li>
<li>Doesn't share features learned across different positions of text.</li>
</ul>
<h2>Recurrent Neural Networks</h2>
<p>$y^{&lt;i&gt;}$ not only depends on the current input$x^{&lt;i&gt;}$, but also depends on former inputs $x^{&lt;i-1&gt;} \dots x^{1}$.</p>
<p>Each layer passes an activation function $a^{&lt;i&gt;}$ to next layer $&lt;i+1&gt;$, at the beginning, we will need a $a^{&lt;0&gt;}$ which is usually a vector of zeros.</p>
<h3>Uni-direction vs Bi-direction</h3>
<p>There is a problem for this Uni-diretional RNN:</p>
<ul>
<li>The current level output can only consider current and former inputs.</li>
</ul>
<p>But sometimes you have to look at following words, for example:</p>
<ul>
<li>He said, &quot;Teddy Roosevelt was a great President.&quot;</li>
<li>He said, &quot;Teddy bears are on sale!&quot;</li>
</ul>
<p>In this case, the first three words are same, in order to figure out what &quot;Teddy&quot; exactely refers to, you should take the following words like &quot;Roosevelt&quot; or &quot;bears&quot; into considerations. So, there comes a RNN model that considers both former and following words, which is called Bidirectional RNN(BRNN).</p>
<h2>Forward Propagation</h2>
<p>At the ever beginning
$$a^{&lt;0&gt;}=\vec{0}$$
For the first level
$$a^{&lt;1&gt;}=g_1(w_{aa}a^{&lt;0&gt;}+w_{ax}x^{&lt;1&gt;}+b_a)\leftarrow \underline{tanh}\ or\  ReLU\
\hat y^{&lt;1&gt;}=g_2(w_{ya}a^{&lt;1&gt;}+b_y)\leftarrow\ sigmoid$$
More generally
$$a^{&lt;t&gt;}=g_1(w_{aa}a^{&lt;t-1&gt;}+w_{ax}x^{&lt;t&gt;}+b_a)\leftarrow \underline{tanh}\ or\  ReLU\
\hat y^{&lt;t&gt;}=g_2(w_{ya}a^{&lt;t&gt;}+b_y)\leftarrow\ sigmoid$$</p>
<h3>Simplified RNN notation</h3>
<p>You can write the above formula like:
$$a^{&lt;t&gt;}=g_1(w_{a}\binom{a^{&lt;t-1&gt;}}{x{&lt;t&gt;}}+b_a) \ [w_{aa}\ w_{ax}]=w_a$$</p>
<h2>Backpropagation through time</h2>
<p>Usually, a DL framework can automatically take care of backpropagation.</p>
<p>Defining a loss function to finish the backpropogation.</p>
<h1>Different types of RNNs</h1>
<p>When we have length of input equals to length of output, it's called: <strong>Many-to-many architecture</strong>: $T_x=T_y$.</p>
<p>By contrast, for a sentiment classification problem,$x=text\ ;\ y=0/1 \ or 1..5 $, rather than using every input to have an output, you can just let RNN read the whole sentence and get an output at the last step. This model is called <strong>Many-to-one architecture</strong>.</p>
<p>There is also <strong>one-to-one architecture</strong>, which is maybe less interesting.</p>
<p>In the end, you can have <strong>one-to-many architecture</strong> that can be used for music generation. You provide an integer as the gender of music or as a first key, you get a chapiter of music.</p>
<p>Another example is Machine translation, it's another version of many-to-many architecture, but there are two distinguishable parts: &quot;encoder&quot;(pure inputs) and &quot;decoder&quot;(pure outputs).</p>
<h1>Languages model and sequence generation</h1>
<h2>Speech recognition</h2>
<p>For example, when I say</p>
<blockquote>
<p>The apple and pear salad were delicious.</p>
</blockquote>
<p>even for those best speech recognition systems, it's hard to figure out</p>
<ol>
<li>The apple and pair salad</li>
<li>The apple and pear salad</li>
</ol>
<p>A speech recognition system could give a probability to each sentence. If the second one has a higher probability, it will be chosen.</p>
<h2>language modelling with an RNN</h2>
<p><strong>Trainng set</strong>: large corpus of english text.</p>
<p>You need firstly <strong>tokenize</strong> the text.</p>
<p>Cats average 15 hours of sleep a day. EOS =&gt; $y^{&lt;1&gt;}\ y^{&lt;2&gt;}\ y^{&lt;3&gt;}\ ... y^{&lt;9&gt;}$$$&lt;EOS&gt;= end\ of\ sentence$$</p>
<p>The Egyptian Mau is a bread of cat. The special word Mau=&gt;$&lt;UNK&gt;=Unknown$</p>
<h2>RNN model</h2>
<p>At each step of a RNN network, the output should be a probability of all words in a dictionary given last words.</p>
<p>To train this network, the loss function at step t could be $L(\hat{y}^{&lt;t&gt;},y^{&lt;t&gt;})=-\sum{y_i^{&lt;t&gt;}log\hat{y}_i^{&lt;t&gt;}}$. The overall loss is $L=\sum{L^{&lt;t&gt;}(\hat{y}^{&lt;t&gt;},y^{&lt;t&gt;})}$.</p>
<p>With thismodel, we can predict the next word given a set of words.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fowe70ba7uj20qo0f07b4.jpg" alt=""></p>
<h1>Sampling novel sequences</h1>
<h2>Sampling a sequence from a trained RNN</h2>
<h3>Word-level language model</h3>
<p>Use words as inputs to train RNN models</p>
<h3>Character-level language model</h3>
<p>Use characters as inputs to train RNN models
pros:</p>
<ul>
<li>Don't have to worry about UNK
cons:</li>
<li>you end up with much longer sequences, 10-20 words equal 100-200 characters, so it's harder to capture dependencies from former characters</li>
<li>more computationally expensive to train</li>
</ul>
<h1>Vanishing gradients with RNNs</h1>
<p>Just like deep neural networks, it can be quite difficult to let the later time steps affect the earlier computations. But for RNNs, it's possible to have both vanishing and exploding gradients.</p>
<p>A possible solution for exploding gradients is to apply gradient clipping, which means looking at your gradient vector, if it's bigger than some thresholds, resize these gradient vectors.</p>
<h1>Gated Recurrent Unit(GRU)</h1>
<p>GRU is a possible solution to vanishing gradients.</p>
<h2>GRU(simplified)</h2>
<p>For example, there is a sentence &quot;The cat, which already ate... , was full &quot;, you have to memorize it's &quot;cat&quot; or &quot;cats&quot; to decide &quot;was&quot; or &quot;were&quot;.
$$c=memory\ cell$$
$$c^{&lt;t&gt;}=a^{&lt;t&gt;}$$
Then there is a candidate for replacing the memory cell:
$$\tilde{c}^{&lt;t&gt;}=tanh(w_c[c^{&lt;t-1&gt;},x^{&lt;t&gt;}]+b_c)$$
$$\Gamma_u=\sigma(w_u[c^{&lt;t-1&gt;},x^{&lt;t&gt;}]+b_u), u=update$$
The $\Gamma_u$ should be 1 at the word &quot;cat&quot;, 0 at other words.</p>
<p>For a sigmoid function, the result is a number between 0 and 1, and most time it's either 0 or 1. So it's called <strong>Gate</strong>.</p>
<p>Then use the following function to update memory cells:
$$c^{&lt;t&gt;}=\Gamma_u*\tilde{c}^{&lt;t&gt;}+(1-\Gamma_u)*c^{&lt;t-1&gt;}$$</p>
<p><img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1foxli3a7huj20qo0f0ajs.jpg" alt=""></p>
<h2>Full GRU</h2>
<p>In the full version GRU, we add a new gate $\Gamma_r$ in the candidate function, r stands for relevance. The main structure keeps unchanged.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1foxmfgoxgvj20qo0f0tb5.jpg" alt=""></p>
<h1>Long Short term Memory(LSTM)</h1>
<p>Another solution, maybe even more powerful than GRU in some cases, is LSTM. Instead of using two gates, an update, a forget gate and an output gate are used in LSTM.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1foxte03cmaj20qo0f0ah7.jpg" alt="">
At each time step, we use those three gates to update memory cells.</p>
<p>LSTM came out earlier than GRU, but in different cases, they could have different performances.</p>
<h1>Bidirectional RNN</h1>
<p>In the previous models, there are only forwards connections, but in BRNN, there will be also backwards connections.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1foxty66rg6j20qo0f0agb.jpg" alt="">
A Bidirectional structure with LSTM blocks is a first thing to try for recent NLP problems.</p>
<h1>Deep RNNs</h1>
<p>In a deep RNN, we shall use a chained blocks as a layer. So to compute a block, both horizontal and vertical inputs are needed. Like $a^{[2]&lt;3&gt;}=g(w_a^{[2]}[a^{[2]&lt;2&gt;},a^{[1]&lt;3&gt;}]+b_a^{[3]})$
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1foxu8xx22fj20qo0f0grx.jpg" alt=""></p>
<p>Usually there aren't many horizontal layers, but we add some vertical connections at the last layer outputs.</p>
]]></content>
      
        <categories>
            
            <category> Coursera </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> NLP </tag>
            
        </tags>
        
    </entry>
    
  
  
</search>
