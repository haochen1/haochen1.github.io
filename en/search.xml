<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title><![CDATA[Running Jupyter Notebook on Google Cloud for a Kaggle challenge]]></title>
      <url>/en/2018/04/18/Jupyter%20on%20GCP/</url>
      <content type="html"><![CDATA[<p>When you start a Kaggle challenge, a computer is usually needed to hold all dataset in the memory and accelerate the training with your GPU. Rather than purchasing a new computer,  I'd like to do it free with 300$ credit offered by Google Cloud Platform.</p>
<h1>Step 1: Create a free account in Google Cloud</h1>
<p>For this step, you can create a new Google Account or sign in with your Google Account on https://cloud.google.com/. Then, you will have to put your payment information and verify your account.</p>
<h1>Step 2 : Create a new project on GCP</h1>
<p>Click on the three dots shown in the image below and then click on the + sign to create a new project.
<img src="http://ww1.sinaimg.cn/large/b66c02d5gy1fqgzqonxylj20zn0jymys.jpg" alt=""></p>
<h1>Step 3 : Create a VM instance</h1>
<p>Click on the three lines on the upper left corner, then on the compute option, click on ‘Compute Engine’</p>
<p><img src="http://ww1.sinaimg.cn/large/b66c02d5gy1fqgzurimk3j20ij09vgm2.jpg" alt=""></p>
<p>Now click on ‘Create new instance’. Name your instance, select a zone close to you, in my case, I chose 'europe-west1-b' .Choose your ‘machine type’. (I chose 8v CPUs 52 GB memory because i had a huge dataset). GCP will give you a estimated price according to your configurations.</p>
<p><img src="http://ww1.sinaimg.cn/large/b66c02d5gy1fqh00w2994j20rh09dq3g.jpg" alt=""></p>
<p>You can also customize your vitual machine if you need GPUs. Attention, GPUs are available only in several zones. So , make sure that you have chosen a zone from below:</p>
<ul>
<li>us-west1-b</li>
<li>us-central1-c</li>
<li>us-central1-f</li>
<li>us-east1-c</li>
<li>europe-west1-b</li>
<li>europe-west1-d</li>
<li>asia-east1-a</li>
<li>asia-east1-c</li>
<li>europe-west4-a</li>
</ul>
<p><img src="http://ww1.sinaimg.cn/large/b66c02d5gy1fqh09m85ddj20d70gcq3q.jpg" alt=""></p>
<p>Under Boot Disk option, select your os as ‘Ubuntu 16.04 LTS’ and your disk size as what you need for your datasets, for example, I need 50 GB.</p>
<p><img src="http://ww1.sinaimg.cn/large/b66c02d5gy1fqh0hj7da8j20m70r5goz.jpg" alt=""></p>
<p>Under the firewall options tick both ‘http’ and ‘https’ (very important). Then, choose the disk tab and untick ‘ Delete boot disk when instance is deleted’.</p>
<p><img src="http://ww1.sinaimg.cn/large/b66c02d5gy1fqh0j71xh6j20ei0eyjs4.jpg" alt=""></p>
<p>Now click on ‘Create’ and your instance is ready!</p>
<p><img src="http://ww1.sinaimg.cn/large/b66c02d5gy1fqh35wku3uj20ur0cjt9a.jpg" alt=""></p>
<p><strong>IMPORTANT : DON’T FORGET TO STOP YOUR GPU INSTANCE AFTER YOU ARE DONE BY CLICKING ON THE THREE DOTS ON THE IMAGE ABOVE AND SELECTING STOP. OTHERWISE GCP WILL KEEP CHARGING YOU ON AN HOURLY BASIS.</strong></p>
<h1>Step 4: Make external IP address as static</h1>
<p>By default, the external IP address is dynamic and we need to make it static to make our life easier. Click on the three horizontal lines on top left and then under networking, click on VPC network and then External IP addresses.</p>
<p><img src="http://ww1.sinaimg.cn/large/b66c02d5gy1fqh3esd116j20cn0n13zp.jpg" alt=""></p>
<p>Change the type from Ephemeral to Static.</p>
<p><img src="http://ww1.sinaimg.cn/large/b66c02d5gy1fqh3i1zgulj20og03ydfu.jpg" alt=""></p>
<p>Now, click on the ‘Firewall rules’ setting under VPC network. Click on ‘Create Firewall Rules’ and refer the below image:</p>
<p><img src="http://ww1.sinaimg.cn/large/b66c02d5gy1fqh3pd7lmjj20gi0owmxz.jpg" alt=""></p>
<p>Under protocols and ports you can choose any port. I have chosen tcp:5000 as my port number. Now click on the save button.</p>
<h1>Step 5 : Install Google Cloud SDK</h1>
<p>According to your OS, refer the corresponding document on https://cloud.google.com/sdk/docs/quickstarts</p>
<p>Then run
<code>gcloud init</code>
follow steps on yhe website to initialize your Google Cloud SDK.</p>
<h1>Step 6 : Install Jupyter notebook and other packages</h1>
<p>Open a terminal, connect to your VM instance:</p>
<p><code>gcloud compute --project &lt;project name&gt; ssh --zone &lt;zone name&gt; &lt;instance name&gt;</code></p>
<p>Then, install anaconda3 into your VM,</p>
<pre><code>wget http://repo.continuum.io/archive/Anaconda3-5.1.0-Linux-x86_64.sh	

bash Anaconda3-4.0.0-Linux-x86_64.sh
</code></pre>
<p>and follow the on-screen instructions. The defaults usually work fine, but answer yes to the last question about prepending the install location to PATH:</p>
<pre><code>Do you wish the installer to prepend the 
Anaconda3 install location to PATH 
in your /home/haroldsoh/.bashrc ? 
[yes|no][no] &gt;&gt;&gt; yes
</code></pre>
<p>To make use of Anaconda right away, source your bashrc:</p>
<pre><code>source ~/.bashrc
</code></pre>
<p>Now, install other softwares, for example</p>
<pre><code>conda install -c conda-forge lightgbm 
</code></pre>
<h1>Step 7: Set up the VM server</h1>
<p>Open up a SSH session to your VM. Check if you have a Jupyter configuration file:</p>
<pre><code>ls ~/.jupyter/jupyter_notebook_config.py
</code></pre>
<p>If it doesn’t exist, create one:</p>
<pre><code>jupyter notebook --generate-config
</code></pre>
<p>We’re going to add a few lines to your Jupyter configuration file; the file is plain text so, you can do this via your favorite editor (e.g., vim, emacs). Make sure you replace the port number with the one you allowed firewall access to in step 4.</p>
<pre><code>c = get_config()
c.NotebookApp.ip = '*'
c.NotebookApp.open_browser = False
c.NotebookApp.port = &lt;Port Number&gt;
</code></pre>
<p>It should look something like this :</p>
<p><img src="http://ww1.sinaimg.cn/large/b66c02d5gy1fqh4q4uy1yj20s40jgmyq.jpg" alt=""></p>
<h1>Step 8 : Launching Jupyter Notebook</h1>
<p>To run the jupyter notebook, just type the following command in the ssh window you are in :</p>
<pre><code>jupyter notebook --ip=0.0.0.0 --port=&lt;port-number&gt; --no-browser &amp;
</code></pre>
<p>Once you run the command, it gives you a token like this:</p>
<p><img src="http://ww1.sinaimg.cn/large/b66c02d5gy1fqh4x9syeaj20k20a3ac6.jpg" alt=""></p>
<p>Now to launch your jupyter notebook, just type the following in your browser :</p>
<pre><code>http://&lt;External Static IP Address&gt;:&lt;Port Number&gt;
</code></pre>
<p>where, external ip address is the ip address which we made static and port number is the one which we allowed firewall access to.</p>
<p>Enter the token you got in the last step:</p>
<p><img src="http://ww1.sinaimg.cn/large/b66c02d5gy1fqh57tvcx7j20rl0mxgn4.jpg" alt=""></p>
<p>Then you have a jupyter notebook running on GCP.</p>
<p><img src="http://ww1.sinaimg.cn/large/b66c02d5gy1fqh5amoxfgj21az0f2q3f.jpg" alt=""></p>
<h1>References</h1>
<ol>
<li>
<p><a href="https://towardsdatascience.com/running-jupyter-notebook-in-google-cloud-platform-in-15-min-61e16da34d52" target="_blank" rel="noopener">Running Jupyter Notebook on Google Cloud Platform in 15 min</a></p>
</li>
<li>
<p><a href="https://cloud.google.com/sdk/docs/quickstarts" target="_blank" rel="noopener">Google Cloud Quickstarts</a></p>
</li>
</ol>
]]></content>
      
        <categories>
            
            <category> Kaggle </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Google Cloud Platform </tag>
            
            <tag> Kaggle </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Sequence Model week3]]></title>
      <url>/en/2018/03/15/Sequence-Model-week3/</url>
      <content type="html"><![CDATA[<h1>Various sequence to sequence qrchitectures</h1>
<h2>Basic Models</h2>
<h3>Machine translation</h3>
<p>For a machine tanslation, you need a encoder-decoder model. In this model, each block can represent a RNN/LSTM/GRU.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fpbj7k98blj20qo0f0gq5.jpg" alt=""></p>
<h3>Image captioning</h3>
<p>How to interpret an image into a caption? In the example, we use a pre-trainde Alex network as encoders. Then we also need sevaral RNN blocks as decoders.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fpbjl2nlhpj20qo0f0dmd.jpg" alt=""></p>
<h2>Picking the most likely sentence</h2>
<p>A machine translation problem can be regarded as a conditional language model, and &quot;conditional&quot; means given the probability of the original sentence.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fpbkgx0oo6j20qo0f0gr4.jpg" alt=""></p>
<p>###Why not a greedy search?
<strong>greedy search:</strong> find the most likely word for the position 1, then for the position 2, etc..
It doesn't work well when we need to translate a whole sentence.</p>
<h2>Beam Search</h2>
<h3>step 1</h3>
<p>B=3 means that 3 most likely words are chosen.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fpblyt59uwj20qo0f0ad0.jpg" alt=""></p>
<h3>step 2</h3>
<p>For each candidate word chosen in step 1, run RNN blocks to find out the most likely pairs of $y^{&lt;1&gt;},y^{&lt;2&gt;}$, then cut down these 30000 possibilities into 3 most possible.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fpbmkcxpj0j20qo0f0gv4.jpg" alt=""></p>
<h3>step 3</h3>
<p>Same pocesse like step 2.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fpbml4fkw6j20qo0f0jw6.jpg" alt=""></p>
<h2>Refinements to Beam Search</h2>
<h3>Length normalization</h3>
<p>$P(y^{&lt;1&gt;}...y^{&lt;T_y&gt;}|x)$ can be thought as a product of many conditional probabilities, which can be too small.</p>
<p>The result of model tends to be short sentences, because the product of probability of short sentence is likely to be bigger.</p>
<p>To avoid this disadvantage, one approach is dividing this product by number of words in the sentence.
<img src="http://ww1.sinaimg.cn/large/b66c02d5gy1fpcwk14z1lj20qo0f0wjq.jpg" alt=""></p>
<h3>Beam search discussion</h3>
<p>Beam width B is a compromise of performance and speed.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fpcwmzdh8tj20qo0f0gpn.jpg" alt=""></p>
<h2>Error analysis in beam search</h2>
<p>When there is an error, we want to be clear this error comes from RNN or beam search.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fpdsgsmf59j20qo0f0tez.jpg" alt=""></p>
<h2>Blue Score</h2>
<p>There may be several possible translation for a sentence.  How to choose one? A way is to count the max number of words that appear in the reference.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fpdvn5qrkwj20qo0f0teo.jpg" alt=""></p>
<p>Blue score on bigrams
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fpdwaxawzsj20qo0f0q7m.jpg" alt=""></p>
<p>Blue score on unigrams
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fpdwcv6isoj20qo0f0wki.jpg" alt=""></p>
<p>BP= brevity penalty
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fpdwlzjnqbj20qo0f0tcu.jpg" alt=""></p>
<p>##Attention model Intuition
Encoder-decoder model can lose performance when a sentence is too long.</p>
<p>In a bidirectional RNN, you can add an attention weight to each word, which measures how much attention you should pay attention to this word. Then these weights are sent to another RNN to evaluate attentions.</p>
<p><img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fpdxp784xwj20qo0f0gst.jpg" alt=""></p>
<p>More clearly, $\alpha^{&lt;t,t\prime&gt;}= amount\ of\ attention\ that\ y^{&lt;t&gt;}\ should\ pay\ to\ a^{&lt;t\prime&gt;}$</p>
<p><img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fpe09cy2tvj20qo0f0n4a.jpg" alt=""></p>
<h3>Computing attention</h3>
<p>Use a small NN to get proper e
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fpe0gosl03j20qo0f0tet.jpg" alt=""></p>
<h1>Speech recognition - Audio Data</h1>
<h2>Speech recognition</h2>
<p>We can also apply an attention model to speech recognition.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fpetgs5x0kj20qo0f00v5.jpg" alt=""></p>
<p>Another approach is Connectionist temporal classification.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fpetoydz9bj20qo0f0jwm.jpg" alt=""></p>
<h2>Trigger Word Detection</h2>
<p>Use 0/1 to distinguish trigger words or nornam words.</p>
<p><img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fpeu5v60t2j20qo0f0dk9.jpg" alt=""></p>
]]></content>
      
        <categories>
            
            <category> Coursera </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> NLP </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Sequence Model week2]]></title>
      <url>/en/2018/03/10/Sequence-Model-week2/</url>
      <content type="html"><![CDATA[<h1>Introduction to Word Embeddings</h1>
<h2>Word Representation</h2>
<h3>One-hot representation</h3>
<p>So far, we have used a vocabulary and one-hot representation
$$Man(5391)= \begin{bmatrix} 0\\ \vdots \\ 0 \\ 1\\0\\ \vdots\\ 0 \end{bmatrix}=O_{5391}$$
$$Woman(9853)= \begin{bmatrix} 0\\ \vdots \\ 0 \\ 1\\0\\ \vdots\\ 0 \end{bmatrix}=O_{9853}$$
It's hard to recognize the relationship between 2 vectors, because inner product of any 2 one-hot vectors is 0.</p>
<h3>Featurized representation: word embedding</h3>
<p>Instead of using one-hot encoding, to represent a word, you can choose some features like Gender, Royal, Age, Food .. Size, Cost.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fp3d8i1h66j20qo0f0n3b.jpg" alt=""></p>
<p>##Using word embeddings
Using transfer learning and word embeddings can be a good approach to slove NLP problems.</p>
<p>Here are the 3 steps that you need to do when considering this approach.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fp3eyj4vo1j20qo0f0q7y.jpg" alt=""></p>
<h2>Properties of word embeddings</h2>
<p>One interesting property is if we take two vectors like $e_{man}$ and $e_{woman}$, $e_{man}-e_{woman}$ can give the main difference between 2 vectors.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fp4h09qz12j20qo0f0dm3.jpg" alt="">
To put this into an algorithm, for example, if you want to find what is the corresponding word of &quot;king&quot; in the way like &quot;man&quot; and &quot;woman&quot;, we need to calculate the similarity.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fp4h5mkc70j20qo0f0n1d.jpg" alt="">
Two similarity calculation methods:</p>
<ol>
<li>** Cosine similarity **</li>
<li>** Euclidian distance **
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fp4hp1pirjj20qo0f0adr.jpg" alt=""></li>
</ol>
<h2>Embedding Matrix</h2>
<p>You can put all Use an embeddinng matrix and one-hot vector can get an embedding vector easily.</p>
<p>But in practice, use special function to look up an embedding.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fp4hytdamvj20qo0f0wkm.jpg" alt=""></p>
<h1>Learning Word Embedddings: Word2vec &amp; GloVe</h1>
<h2>Learning word embedddings</h2>
<p>A learning model can be like: word embedding =&gt; neural network with parameters $w^{[1]}$ and $b^{[1]}$ =&gt; softmax with parameters $w^{[2]}$ and $b^{[2]}$</p>
<p>A common algorithm for learning the matrix E looks like the following image, we want to predict the target word &quot;juice&quot; using context words&quot;a glass of orange&quot;.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fp4l154hc6j20qo0f0grx.jpg" alt="">
Context can vary from last few words to next few words or both. Pairs of context/target are import</p>
<h2>Word2Vec Skip-gram model</h2>
<p>Word2Vec is a simple way to learn word embeddings.</p>
<p>When apply Skip-grams for context/target selection, we randomly pick up words in a window as context/target.</p>
<p><img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fp4osyx745j20qo0f078i.jpg" alt=""></p>
<p>A problem with softmax classification, every time you need a propobility of a target word, you calculate a sum whose length equals to that of vocabulary. When you have many words in the vocabulary, it can be very slow.</p>
<p>So instead of using softmax classification, hierarchical softmax can be a more proper way. You can use a tree to figure out that a word locates in which part of a vocabulary.</p>
<p>For instance, the target can be chosen within a windows of -10/10 words. But choosing a context can be more tricky, you should avoid using frequent non-sense words like &quot;the&quot; &quot;and&quot; &quot;to&quot; &quot;a&quot;.</p>
<p>A even simpler solution to this problem is Negative Sampling.</p>
<h2>Negative Sampling</h2>
<p>We create a supervised problem, given pairs of context/target as x, y should matching results in 0/1. To form a dataset, 5-20 pairs for small datasets and 2-5 pairs for larger datasets.</p>
<p><img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fp4qxi3hxvj20qo0f0dkw.jpg" alt=""></p>
<p>We only train the model with 1 correct pair and 4 other incorrect pairs.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fp4r8vaxjsj20qo0f0jw0.jpg" alt=""></p>
<p>To select negative samples, the authur proposed a way:
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fp4rb9bz9qj20qo0f0goi.jpg" alt=""></p>
<h2>GloVe word vectors</h2>
<p>We define pairs of context/target in the following way:
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fp5k889htuj20qo0f077t.jpg" alt=""></p>
<p>The model looks like:
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fp5ki84tauj20qo0f0q68.jpg" alt="">
You can't guarantee that an axis is well aligned to a feature.</p>
<p>#Allications using Word Embeddings</p>
<h2>Sentiment Classification</h2>
<p>A simple sentiment classification model takes the average of each word embedding vector.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fp5liyzosxj20qo0f0q7i.jpg" alt=""></p>
<p>Use a RNN can avoid disadvantages of some special sentences.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fp5llahpgvj20qo0f0whs.jpg" alt=""></p>
<h2>Debiasing word embeddings</h2>
<p>What is bias in word embedding?
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fp5pnb2omxj20qo0f0wjg.jpg" alt="">
<img src="http://ww1.sinaimg.cn/large/b66c02d5gy1fp5rtzo750j20qo0f00yd.jpg" alt=""></p>
]]></content>
      
        <categories>
            
            <category> Coursera </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> NLP </tag>
            
        </tags>
        
    </entry>
    
    <entry>
      <title><![CDATA[Sequence Model week1]]></title>
      <url>/en/2018/03/05/Sequence-Model-week1/</url>
      <content type="html"><![CDATA[<h1>Why sequence models</h1>
<h2>Examples of sequence data</h2>
<ol>
<li>speech recognition</li>
<li>Music generation</li>
<li>Sentiment classification</li>
<li>DNA sequence analysis</li>
<li>Machine translation</li>
<li>Video activity recognition</li>
<li>Name entity recognition</li>
</ol>
<h1>Notation</h1>
<h2>Motivating example</h2>
<p>Name entity recognition</p>
<table>
<thead>
<tr>
<th>x</th>
<th>Harry</th>
<th>Potter</th>
<th>and</th>
<th>Hermione</th>
<th>Granger</th>
<th>invented</th>
<th>a</th>
<th>new</th>
<th>spell.</th>
</tr>
</thead>
<tbody>
<tr>
<td>y</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>$$x^{&lt;1&gt;} \to x^{&lt;9&gt;} stand\ for\ the\ 1st\ to\ 9th\ element\ in\ x\ vector, T_x=9\ length=9$$</p>
<p>$$y^{&lt;1&gt;} \to y^{&lt;9&gt;} stand\ for\ the\ 1st\ to\ 9th\ element\ in\ y\ vector, T_y=9\ length =9$$</p>
<p>$x^{(i)&lt;t&gt;}$ refers to $(i)$th training example's $&lt;t&gt;$th element.</p>
<p>$T_x^{(i)}$ would be the input sequence length for training example i.</p>
<h2>Representing words</h2>
<p>To represent words in a sentence, firstly we need a Vocabulary/Dictionary which is a list of the words that you will use in your representations.</p>
<table>
<thead>
<tr>
<th>a</th>
<th>aaron</th>
<th>...</th>
<th>and</th>
<th>...</th>
<th>harry</th>
<th>...</th>
<th>potter</th>
<th>...</th>
<th>zulu</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>2</td>
<td>...</td>
<td>367</td>
<td>...</td>
<td>4075</td>
<td>...</td>
<td>6830</td>
<td>...</td>
<td>10000</td>
</tr>
</tbody>
</table>
<p>Use one-hot encoding</p>
<p>$$x^{&lt;1&gt;}= \begin{bmatrix}  0\\   \vdots \\   0\\ 1\leftarrow4075\\ 0\\   \vdots  \\  0   \end{bmatrix};
x^{&lt;2&gt;}=\begin{bmatrix}  0\\   \vdots \\   0\\ 1\leftarrow6830\\ 0\\   \vdots  \\  0   \end{bmatrix};
x^{&lt;3&gt;}=\begin{bmatrix}  0\\   \vdots \\   0\\ 1\leftarrow367\\ 0\\   \vdots  \\  0   \end{bmatrix};
x^{&lt;7&gt;}=\begin{bmatrix}  1\\   \vdots \\   0\\ 0\\ 0\\   \vdots  \\  0   \end{bmatrix}$$
When a word can't be found in the Vocabulary, we shall add $&lt;UNK&gt;$ to represent this word.</p>
<h1>Recurrent Neural Networks</h1>
<h2>Why not a standard network?</h2>
<p>Some problems:</p>
<ul>
<li>Inputs, outputs can be different lengths in different examples.</li>
<li>Doesn't share features learned across different positions of text.</li>
</ul>
<h2>Recurrent Neural Networks</h2>
<p>$y^{&lt;i&gt;}$ not only depends on the current input$x^{&lt;i&gt;}$, but also depends on former inputs $x^{&lt;i-1&gt;} \dots x^{1}$.</p>
<p>Each layer passes an activation function $a^{&lt;i&gt;}$ to next layer $&lt;i+1&gt;$, at the beginning, we will need a $a^{&lt;0&gt;}$ which is usually a vector of zeros.</p>
<h3>Uni-direction vs Bi-direction</h3>
<p>There is a problem for this Uni-diretional RNN:</p>
<ul>
<li>The current level output can only consider current and former inputs.</li>
</ul>
<p>But sometimes you have to look at following words, for example:</p>
<ul>
<li>He said, &quot;Teddy Roosevelt was a great President.&quot;</li>
<li>He said, &quot;Teddy bears are on sale!&quot;</li>
</ul>
<p>In this case, the first three words are same, in order to figure out what &quot;Teddy&quot; exactely refers to, you should take the following words like &quot;Roosevelt&quot; or &quot;bears&quot; into considerations. So, there comes a RNN model that considers both former and following words, which is called Bidirectional RNN(BRNN).</p>
<h2>Forward Propagation</h2>
<p>At the ever beginning
$$a^{&lt;0&gt;}=\vec{0}$$
For the first level
$$a^{&lt;1&gt;}=g_1(w_{aa}a^{&lt;0&gt;}+w_{ax}x^{&lt;1&gt;}+b_a)\leftarrow \underline{tanh}\ or\  ReLU\
\hat y^{&lt;1&gt;}=g_2(w_{ya}a^{&lt;1&gt;}+b_y)\leftarrow\ sigmoid$$
More generally
$$a^{&lt;t&gt;}=g_1(w_{aa}a^{&lt;t-1&gt;}+w_{ax}x^{&lt;t&gt;}+b_a)\leftarrow \underline{tanh}\ or\  ReLU\
\hat y^{&lt;t&gt;}=g_2(w_{ya}a^{&lt;t&gt;}+b_y)\leftarrow\ sigmoid$$</p>
<h3>Simplified RNN notation</h3>
<p>You can write the above formula like:
$$a^{&lt;t&gt;}=g_1(w_{a}\binom{a^{&lt;t-1&gt;}}{x{&lt;t&gt;}}+b_a) \ [w_{aa}\ w_{ax}]=w_a$$</p>
<h2>Backpropagation through time</h2>
<p>Usually, a DL framework can automatically take care of backpropagation.</p>
<p>Defining a loss function to finish the backpropogation.</p>
<h1>Different types of RNNs</h1>
<p>When we have length of input equals to length of output, it's called: <strong>Many-to-many architecture</strong>: $T_x=T_y$.</p>
<p>By contrast, for a sentiment classification problem,$x=text\ ;\ y=0/1 \ or 1..5 $, rather than using every input to have an output, you can just let RNN read the whole sentence and get an output at the last step. This model is called <strong>Many-to-one architecture</strong>.</p>
<p>There is also <strong>one-to-one architecture</strong>, which is maybe less interesting.</p>
<p>In the end, you can have <strong>one-to-many architecture</strong> that can be used for music generation. You provide an integer as the gender of music or as a first key, you get a chapiter of music.</p>
<p>Another example is Machine translation, it's another version of many-to-many architecture, but there are two distinguishable parts: &quot;encoder&quot;(pure inputs) and &quot;decoder&quot;(pure outputs).</p>
<h1>Languages model and sequence generation</h1>
<h2>Speech recognition</h2>
<p>For example, when I say</p>
<blockquote>
<p>The apple and pear salad were delicious.</p>
</blockquote>
<p>even for those best speech recognition systems, it's hard to figure out</p>
<ol>
<li>The apple and pair salad</li>
<li>The apple and pear salad</li>
</ol>
<p>A speech recognition system could give a probability to each sentence. If the second one has a higher probability, it will be chosen.</p>
<h2>language modelling with an RNN</h2>
<p><strong>Trainng set</strong>: large corpus of english text.</p>
<p>You need firstly <strong>tokenize</strong> the text.</p>
<p>Cats average 15 hours of sleep a day. EOS =&gt; $y^{&lt;1&gt;}\ y^{&lt;2&gt;}\ y^{&lt;3&gt;}\ ... y^{&lt;9&gt;}$$$&lt;EOS&gt;= end\ of\ sentence$$</p>
<p>The Egyptian Mau is a bread of cat. The special word Mau=&gt;$&lt;UNK&gt;=Unknown$</p>
<h2>RNN model</h2>
<p>At each step of a RNN network, the output should be a probability of all words in a dictionary given last words.</p>
<p>To train this network, the loss function at step t could be $L(\hat{y}^{&lt;t&gt;},y^{&lt;t&gt;})=-\sum{y_i^{&lt;t&gt;}log\hat{y}_i^{&lt;t&gt;}}$. The overall loss is $L=\sum{L^{&lt;t&gt;}(\hat{y}^{&lt;t&gt;},y^{&lt;t&gt;})}$.</p>
<p>With thismodel, we can predict the next word given a set of words.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fowe70ba7uj20qo0f07b4.jpg" alt=""></p>
<h1>Sampling novel sequences</h1>
<h2>Sampling a sequence from a trained RNN</h2>
<h3>Word-level language model</h3>
<p>Use words as inputs to train RNN models</p>
<h3>Character-level language model</h3>
<p>Use characters as inputs to train RNN models
pros:</p>
<ul>
<li>Don't have to worry about UNK
cons:</li>
<li>you end up with much longer sequences, 10-20 words equal 100-200 characters, so it's harder to capture dependencies from former characters</li>
<li>more computationally expensive to train</li>
</ul>
<h1>Vanishing gradients with RNNs</h1>
<p>Just like deep neural networks, it can be quite difficult to let the later time steps affect the earlier computations. But for RNNs, it's possible to have both vanishing and exploding gradients.</p>
<p>A possible solution for exploding gradients is to apply gradient clipping, which means looking at your gradient vector, if it's bigger than some thresholds, resize these gradient vectors.</p>
<h1>Gated Recurrent Unit(GRU)</h1>
<p>GRU is a possible solution to vanishing gradients.</p>
<h2>GRU(simplified)</h2>
<p>For example, there is a sentence &quot;The cat, which already ate... , was full &quot;, you have to memorize it's &quot;cat&quot; or &quot;cats&quot; to decide &quot;was&quot; or &quot;were&quot;.
$$c=memory\ cell$$
$$c^{&lt;t&gt;}=a^{&lt;t&gt;}$$
Then there is a candidate for replacing the memory cell:
$$\tilde{c}^{&lt;t&gt;}=tanh(w_c[c^{&lt;t-1&gt;},x^{&lt;t&gt;}]+b_c)$$
$$\Gamma_u=\sigma(w_u[c^{&lt;t-1&gt;},x^{&lt;t&gt;}]+b_u), u=update$$
The $\Gamma_u$ should be 1 at the word &quot;cat&quot;, 0 at other words.</p>
<p>For a sigmoid function, the result is a number between 0 and 1, and most time it's either 0 or 1. So it's called <strong>Gate</strong>.</p>
<p>Then use the following function to update memory cells:
$$c^{&lt;t&gt;}=\Gamma_u*\tilde{c}^{&lt;t&gt;}+(1-\Gamma_u)*c^{&lt;t-1&gt;}$$</p>
<p><img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1foxli3a7huj20qo0f0ajs.jpg" alt=""></p>
<h2>Full GRU</h2>
<p>In the full version GRU, we add a new gate $\Gamma_r$ in the candidate function, r stands for relevance. The main structure keeps unchanged.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1foxmfgoxgvj20qo0f0tb5.jpg" alt=""></p>
<h1>Long Short term Memory(LSTM)</h1>
<p>Another solution, maybe even more powerful than GRU in some cases, is LSTM. Instead of using two gates, an update, a forget gate and an output gate are used in LSTM.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1foxte03cmaj20qo0f0ah7.jpg" alt="">
At each time step, we use those three gates to update memory cells.</p>
<p>LSTM came out earlier than GRU, but in different cases, they could have different performances.</p>
<h1>Bidirectional RNN</h1>
<p>In the previous models, there are only forwards connections, but in BRNN, there will be also backwards connections.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1foxty66rg6j20qo0f0agb.jpg" alt="">
A Bidirectional structure with LSTM blocks is a first thing to try for recent NLP problems.</p>
<h1>Deep RNNs</h1>
<p>In a deep RNN, we shall use a chained blocks as a layer. So to compute a block, both horizontal and vertical inputs are needed. Like $a^{[2]&lt;3&gt;}=g(w_a^{[2]}[a^{[2]&lt;2&gt;},a^{[1]&lt;3&gt;}]+b_a^{[3]})$
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1foxu8xx22fj20qo0f0grx.jpg" alt=""></p>
<p>Usually there aren't many horizontal layers, but we add some vertical connections at the last layer outputs.</p>
]]></content>
      
        <categories>
            
            <category> Coursera </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> NLP </tag>
            
        </tags>
        
    </entry>
    
  
  
</search>
