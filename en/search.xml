<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title><![CDATA[Sequence Model week1]]></title>
      <url>/en/2018/03/05/Sequence-Model-week1/</url>
      <content type="html"><![CDATA[<h1>Why sequence models</h1>
<h2>Examples of sequence data</h2>
<ol>
<li>speech recognition</li>
<li>Music generation</li>
<li>Sentiment classification</li>
<li>DNA sequence analysis</li>
<li>Machine translation</li>
<li>Video activity recognition</li>
<li>Name entity recognition</li>
</ol>
<h1>Notation</h1>
<h2>Motivating example</h2>
<p>Name entity recognition</p>
<table>
<thead>
<tr>
<th>x</th>
<th>Harry</th>
<th>Potter</th>
<th>and</th>
<th>Hermione</th>
<th>Granger</th>
<th>invented</th>
<th>a</th>
<th>new</th>
<th>spell.</th>
</tr>
</thead>
<tbody>
<tr>
<td>y</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>$$x^{&lt;1&gt;} \to x^{&lt;9&gt;} stand\ for\ the\ 1st\ to\ 9th\ element\ in\ x\ vector, T_x=9\ length=9$$</p>
<p>$$y^{&lt;1&gt;} \to y^{&lt;9&gt;} stand\ for\ the\ 1st\ to\ 9th\ element\ in\ y\ vector, T_y=9\ length =9$$</p>
<p>$x^{(i)&lt;t&gt;}$ refers to $(i)$th training example's $&lt;t&gt;$th element.</p>
<p>$T_x^{(i)}$ would be the input sequence length for training example i.</p>
<h2>Representing words</h2>
<p>To represent words in a sentence, firstly we need a Vocabulary/Dictionary which is a list of the words that you will use in your representations.</p>
<table>
<thead>
<tr>
<th>a</th>
<th>aaron</th>
<th>...</th>
<th>and</th>
<th>...</th>
<th>harry</th>
<th>...</th>
<th>potter</th>
<th>...</th>
<th>zulu</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>2</td>
<td>...</td>
<td>367</td>
<td>...</td>
<td>4075</td>
<td>...</td>
<td>6830</td>
<td>...</td>
<td>10000</td>
</tr>
</tbody>
</table>
<p>Use one-hot encoding</p>
<p>$$x^{&lt;1&gt;}= \begin{bmatrix}  0\\   \vdots \\   0\\ 1\leftarrow4075\\ 0\\   \vdots  \\  0   \end{bmatrix};
x^{&lt;2&gt;}=\begin{bmatrix}  0\\   \vdots \\   0\\ 1\leftarrow6830\\ 0\\   \vdots  \\  0   \end{bmatrix};
x^{&lt;3&gt;}=\begin{bmatrix}  0\\   \vdots \\   0\\ 1\leftarrow367\\ 0\\   \vdots  \\  0   \end{bmatrix};
x^{&lt;7&gt;}=\begin{bmatrix}  1\\   \vdots \\   0\\ 0\\ 0\\   \vdots  \\  0   \end{bmatrix}$$
When a word can't be found in the Vocabulary, we shall add $&lt;UNK&gt;$ to represent this word.</p>
<h1>Recurrent Neural Networks</h1>
<h2>Why not a standard network?</h2>
<p>Some problems:</p>
<ul>
<li>Inputs, outputs can be different lengths in different examples.</li>
<li>Doesn't share features learned across different positions of text.</li>
</ul>
<h2>Recurrent Neural Networks</h2>
<p>$y^{&lt;i&gt;}$ not only depends on the current input$x^{&lt;i&gt;}$, but also depends on former inputs $x^{&lt;i-1&gt;} \dots x^{1}$.</p>
<p>Each layer passes an activation function $a^{&lt;i&gt;}$ to next layer $&lt;i+1&gt;$, at the beginning, we will need a $a^{&lt;0&gt;}$ which is usually a vector of zeros.</p>
<h3>Uni-direction vs Bi-direction</h3>
<p>There is a problem for this Uni-diretional RNN:</p>
<ul>
<li>The current level output can only consider current and former inputs.</li>
</ul>
<p>But sometimes you have to look at following words, for example:</p>
<ul>
<li>He said, &quot;Teddy Roosevelt was a great President.&quot;</li>
<li>He said, &quot;Teddy bears are on sale!&quot;</li>
</ul>
<p>In this case, the first three words are same, in order to figure out what &quot;Teddy&quot; exactely refers to, you should take the following words like &quot;Roosevelt&quot; or &quot;bears&quot; into considerations. So, there comes a RNN model that considers both former and following words, which is called Bidirectional RNN(BRNN).</p>
<h2>Forward Propagation</h2>
<p>At the ever beginning
$$a^{&lt;0&gt;}=\vec{0}$$
For the first level
$$a^{&lt;1&gt;}=g_1(w_{aa}a^{&lt;0&gt;}+w_{ax}x^{&lt;1&gt;}+b_a)\leftarrow \underline{tanh}\ or\  ReLU\
\hat y^{&lt;1&gt;}=g_2(w_{ya}a^{&lt;1&gt;}+b_y)\leftarrow\ sigmoid$$
More generally
$$a^{&lt;t&gt;}=g_1(w_{aa}a^{&lt;t-1&gt;}+w_{ax}x^{&lt;t&gt;}+b_a)\leftarrow \underline{tanh}\ or\  ReLU\
\hat y^{&lt;t&gt;}=g_2(w_{ya}a^{&lt;t&gt;}+b_y)\leftarrow\ sigmoid$$</p>
<h3>Simplified RNN notation</h3>
<p>You can write the above formula like:
$$a^{&lt;t&gt;}=g_1(w_{a}\binom{a^{&lt;t-1&gt;}}{x{&lt;t&gt;}}+b_a) \ [w_{aa}\ w_{ax}]=w_a$$</p>
<h2>Backpropagation through time</h2>
<p>Usually, a DL framework can automatically take care of backpropagation.</p>
<p>Defining a loss function to finish the backpropogation.</p>
<h1>Different types of RNNs</h1>
<p>When we have length of input equals to length of output, it's called: <strong>Many-to-many architecture</strong>: $T_x=T_y$.</p>
<p>By contrast, for a sentiment classification problem,$x=text\ ;\ y=0/1 \ or 1..5 $, rather than using every input to have an output, you can just let RNN read the whole sentence and get an output at the last step. This model is called <strong>Many-to-one architecture</strong>.</p>
<p>There is also <strong>one-to-one architecture</strong>, which is maybe less interesting.</p>
<p>In the end, you can have <strong>one-to-many architecture</strong> that can be used for music generation. You provide an integer as the gender of music or as a first key, you get a chapiter of music.</p>
<p>Another example is Machine translation, it's another version of many-to-many architecture, but there are two distinguishable parts: &quot;encoder&quot;(pure inputs) and &quot;decoder&quot;(pure outputs).</p>
<h1>Languages model and sequence generation</h1>
<h2>Speech recognition</h2>
<p>For example, when I say</p>
<blockquote>
<p>The apple and pear salad were delicious.</p>
</blockquote>
<p>even for those best speech recognition systems, it's hard to figure out</p>
<ol>
<li>The apple and pair salad</li>
<li>The apple and pear salad</li>
</ol>
<p>A speech recognition system could give a probability to each sentence. If the second one has a higher probability, it will be chosen.</p>
<h2>language modelling with an RNN</h2>
<p><strong>Trainng set</strong>: large corpus of english text.</p>
<p>You need firstly <strong>tokenize</strong> the text.</p>
<p>Cats average 15 hours of sleep a day. EOS =&gt; $y^{&lt;1&gt;}\ y^{&lt;2&gt;}\ y^{&lt;3&gt;}\ ... y^{&lt;9&gt;}$$$&lt;EOS&gt;= end\ of\ sentence$$</p>
<p>The Egyptian Mau is a bread of cat. The special word Mau=&gt;$&lt;UNK&gt;=Unknown$</p>
<h2>RNN model</h2>
<p>At each step of a RNN network, the output should be a probability of all words in a dictionary given last words.</p>
<p>To train this network, the loss function at step t could be $L(\hat{y}^{&lt;t&gt;},y^{&lt;t&gt;})=-\sum{y_i^{&lt;t&gt;}log\hat{y}_i^{&lt;t&gt;}}$. The overall loss is $L=\sum{L^{&lt;t&gt;}(\hat{y}^{&lt;t&gt;},y^{&lt;t&gt;})}$.</p>
<p>With thismodel, we can predict the next word given a set of words.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1fowe70ba7uj20qo0f07b4.jpg" alt=""></p>
<h1>Sampling novel sequences</h1>
<h2>Sampling a sequence from a trained RNN</h2>
<h3>Word-level language model</h3>
<p>Use words as inputs to train RNN models</p>
<h3>Character-level language model</h3>
<p>Use characters as inputs to train RNN models
pros:</p>
<ul>
<li>Don't have to worry about UNK
cons:</li>
<li>you end up with much longer sequences, 10-20 words equal 100-200 characters, so it's harder to capture dependencies from former characters</li>
<li>more computationally expensive to train</li>
</ul>
<h1>Vanishing gradients with RNNs</h1>
<p>Just like deep neural networks, it can be quite difficult to let the later time steps affect the earlier computations. But for RNNs, it's possible to have both vanishing and exploding gradients.</p>
<p>A possible solution for exploding gradients is to apply gradient clipping, which means looking at your gradient vector, if it's bigger than some thresholds, resize these gradient vectors.</p>
<h1>Gated Recurrent Unit(GRU)</h1>
<p>GRU is a possible solution to vanishing gradients.</p>
<h2>GRU(simplified)</h2>
<p>For example, there is a sentence &quot;The cat, which already ate... , was full &quot;, you have to memorize it's &quot;cat&quot; or &quot;cats&quot; to decide &quot;was&quot; or &quot;were&quot;.
$$c=memory\ cell$$
$$c^{&lt;t&gt;}=a^{&lt;t&gt;}$$
Then there is a candidate for replacing the memory cell:
$$\tilde{c}^{&lt;t&gt;}=tanh(w_c[c^{&lt;t-1&gt;},x^{&lt;t&gt;}]+b_c)$$
$$\Gamma_u=\sigma(w_u[c^{&lt;t-1&gt;},x^{&lt;t&gt;}]+b_u), u=update$$
The $\Gamma_u$ should be 1 at the word &quot;cat&quot;, 0 at other words.</p>
<p>For a sigmoid function, the result is a number between 0 and 1, and most time it's either 0 or 1. So it's called <strong>Gate</strong>.</p>
<p>Then use the following function to update memory cells:
$$c^{&lt;t&gt;}=\Gamma_u*\tilde{c}^{&lt;t&gt;}+(1-\Gamma_u)*c^{&lt;t-1&gt;}$$</p>
<p><img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1foxli3a7huj20qo0f0ajs.jpg" alt=""></p>
<h2>Full GRU</h2>
<p>In the full version GRU, we add a new gate $\Gamma_r$ in the candidate function, r stands for relevance. The main structure keeps unchanged.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1foxmfgoxgvj20qo0f0tb5.jpg" alt=""></p>
<h1>Long Short term Memory(LSTM)</h1>
<p>Another solution, maybe even more powerful than GRU in some cases, is LSTM. Instead of using two gates, an update, a forget gate and an output gate are used in LSTM.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1foxte03cmaj20qo0f0ah7.jpg" alt="">
At each time step, we use those three gates to update memory cells.</p>
<p>LSTM came out earlier than GRU, but in different cases, they could have different performances.</p>
<h1>Bidirectional RNN</h1>
<p>In the previous models, there are only forwards connections, but in BRNN, there will be also backwards connections.
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1foxty66rg6j20qo0f0agb.jpg" alt="">
A Bidirectional structure with LSTM blocks is a first thing to try for recent NLP problems.</p>
<h1>Deep RNNs</h1>
<p>In a deep RNN, we shall use a chained blocks as a layer. So to compute a block, both horizontal and vertical inputs are needed. Like $a^{[2]&lt;3&gt;}=g(w_a^{[2]}[a^{[2]&lt;2&gt;},a^{[1]&lt;3&gt;}]+b_a^{[3]})$
<img src="http://ww1.sinaimg.cn/mw690/b66c02d5gy1foxu8xx22fj20qo0f0grx.jpg" alt=""></p>
<p>Usually there aren't many horizontal layers, but we add some vertical connections at the last layer outputs.</p>
]]></content>
      
        <categories>
            
            <category> Coursera </category>
            
        </categories>
        
        
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> NLP </tag>
            
        </tags>
        
    </entry>
    
  
  
</search>
